{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJULtPG9guNm"
   },
   "source": [
    "# Phase 1: Modeling - LSTM + Attention Mechanism w/ pitcher batter embeddings\n",
    "\n",
    "**Author:** Joel Klein, Jacob Sauberman, Ben Perkins\n",
    "\n",
    "**Date:** December 7, 2021\n",
    "\n",
    "**Description:** This script loads in MLB Statcast data from every MLB registered pitch since 2017, prepares the pitch data into sequences of pitches by at bat, and then passes in the pitch sequence data to an LSTM w/ attention mechanism algorithm to predict the next pitch type. \n",
    "\n",
    "**Data:** The data is scraped from *baseballsavant.com* for each year and combined together in one large data file. \n",
    "\n",
    "**Scope:** To avoid data leakage, the data can not be randomly split across all years in the training, validation, and test sets. Future pitches cannot be included as inputs within the training data to generate predictions for past pitches in the validation and/or test data. The data from the 2017 and 2018 seasons will be used for training data while the 2019 season is used for validation. 2020 and 2021 seasons will be used for the testing data in the next pitch prediction model. \n",
    "\n",
    "There are many instances in major league baseball where rookie batters and pitchers receive at-bats and there is little or no prior pitch sequence data. In order to make a prediction on next pitch types in the 2019, 2020, and 2021 seasons, the batter and the pitcher need to have a significant amount of recorded at-bats in the 2017 and 2018 seasons. Only batters and pitchers accounting for 90% of at-bats in the 2017 and 2018 seasons were included in scope (441 batters and 512 pitchers) for generating the pitcher and batter embeddings. There will be a 9-dimensional vector embedding used for the batters and a 9-dimensional vector embedding used for the pitchers. All other players in the dataset that are new will have the pitcher and hitter embedding vectors imputed with zeros.\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "**Warnings:** \n",
    "\n",
    "**Outline:** \n",
    "  - Install Libraries\n",
    "  - Global Options\n",
    "  - Set Directories\n",
    "  - Define Functions\n",
    "  - Load Data\n",
    "  - Data Preparation\n",
    "  - Data Splitting\n",
    "  - Data Loaders\n",
    "  - Train Model\n",
    "  - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vwx_cBcmgvIh"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1kmInt5JeQ3"
   },
   "source": [
    "Let's load in the python libraries needed for the this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12670,
     "status": "ok",
     "timestamp": 1639250274902,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "vjGBv3RKg0mS",
    "outputId": "bfbce1b4-39c9-45eb-bfd2-0ad30ff5837b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybaseball\n",
      "  Downloading pybaseball-2.2.1-py3-none-any.whl (415 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▉                               | 10 kB 27.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 20 kB 34.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 30 kB 40.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 40 kB 29.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 51 kB 18.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 61 kB 20.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 71 kB 14.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 81 kB 15.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 92 kB 14.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 102 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 112 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 122 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 133 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 143 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 153 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 163 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 174 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 184 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 194 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 204 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 215 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 225 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 235 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 245 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 256 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 266 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 276 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 286 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 296 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 307 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 317 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 327 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 337 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 348 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 358 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 368 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 378 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 389 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 399 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 409 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 415 kB 15.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (1.4.1)\n",
      "Requirement already satisfied: attrs>=20.3.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (21.2.0)\n",
      "Requirement already satisfied: lxml>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (4.2.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (4.6.3)\n",
      "Requirement already satisfied: requests>=2.18.1 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (1.19.5)\n",
      "Requirement already satisfied: pyarrow>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (3.0.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (3.2.2)\n",
      "Requirement already satisfied: tqdm>=4.50.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (4.62.3)\n",
      "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (1.1.5)\n",
      "Collecting pygithub>=1.51\n",
      "  Downloading PyGithub-1.55-py3-none-any.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 81.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->pybaseball) (2018.9)\n",
      "Collecting deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting pyjwt>=2.0\n",
      "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting pynacl>=1.4.0\n",
      "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 68.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pynacl>=1.4.0->pygithub>=1.51->pybaseball) (1.15.0)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pynacl>=1.4.0->pygithub>=1.51->pybaseball) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub>=1.51->pybaseball) (2.21)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (2021.10.8)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->pygithub>=1.51->pybaseball) (1.13.3)\n",
      "Installing collected packages: pynacl, pyjwt, deprecated, pygithub, pybaseball\n",
      "Successfully installed deprecated-1.2.13 pybaseball-2.2.1 pygithub-1.55 pyjwt-2.3.0 pynacl-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import plotly.express as px\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "# pipelines\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "\n",
    "# random number generator\n",
    "import random\n",
    "\n",
    "# other\n",
    "import IPython\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# pybaseball\n",
    "!pip install pybaseball\n",
    "from pybaseball import playerid_reverse_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWmA2eCigvKw"
   },
   "source": [
    "## Global Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54kWfnpRJoWI"
   },
   "source": [
    "Let's set a few global options for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1639250274903,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "zRn1T74RhC07"
   },
   "outputs": [],
   "source": [
    "# do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set pandas display options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', 2000)\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9HT0qCwhbFN"
   },
   "source": [
    "## Set Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJA0uUHPJws2"
   },
   "source": [
    "Let's set the directories needed to load in the data into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17407,
     "status": "ok",
     "timestamp": 1639250292304,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "y7rTjs1jhen9",
    "outputId": "2456dfe6-07c4-41e4-a2b3-918934d77751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount data drive on colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# set folder directories to load and save data\n",
    "DATA_DIR = \"/content/drive/MyDrive/final-project-dl/data\"\n",
    "STATCAST_DATA_DIR = \"/content/drive/MyDrive/final-project-dl/data/statcast\"\n",
    "EMBEDDINGS_DATA_DIR = \"/content/drive/MyDrive/final-project-dl/data/embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbbO4WK7hsWT"
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4lWO6_-J2Xw"
   },
   "source": [
    "All custom functions needed in the notebook are listed in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1639250292305,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "BYBm_Yq9hvMX"
   },
   "outputs": [],
   "source": [
    "##### Define data loading function ----\n",
    "\n",
    "# define a function for loading in dataset\n",
    "def load_data(in_path, name):\n",
    "    df = pd.read_csv(in_path)\n",
    "    print(f\"{name}: shape is {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1639250292305,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "pXmSrpKh5XMx"
   },
   "outputs": [],
   "source": [
    "##### Define early stopping class -----\n",
    "\n",
    "# source: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "# date accessed: 2021-12-08\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4MStBqLhgvJ"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7iZIszaZfR9"
   },
   "source": [
    "Let's load in the pitch data from the 2017, 2018, 2019, 2020, and 2021 seasons via the train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22246,
     "status": "ok",
     "timestamp": 1639250315314,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "k3oo9FGehf5B",
    "outputId": "59790d06-22c9-49e6-bf9e-25b0cee46e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: shape is (785777, 107)\n",
      "validation_data: shape is (344941, 107)\n",
      "test_data: shape is (267635, 107)\n"
     ]
    }
   ],
   "source": [
    "# load in each dataset\n",
    "train_data = load_data(os.path.join(STATCAST_DATA_DIR, 'train_data.csv'), 'train_data')\n",
    "validation_data = load_data(os.path.join(STATCAST_DATA_DIR, 'validation_data.csv'), 'validation_data')\n",
    "test_data = load_data(os.path.join(STATCAST_DATA_DIR, 'test_data.csv'), 'test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3VaEa8lEv1n"
   },
   "source": [
    "Let's load in the batter and pitcher embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1639250316166,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "SMAag3jiE52w"
   },
   "outputs": [],
   "source": [
    "# load in batter embeddings reference table\n",
    "with open(os.path.join(EMBEDDINGS_DATA_DIR, 'batter_embedding_df.pkl'), 'rb') as f:\n",
    "    batter_embedding_df = pickle.load(f)\n",
    "\n",
    "# load in pitcher embeddings reference table\n",
    "with open(os.path.join(EMBEDDINGS_DATA_DIR, 'pitcher_embedding_df.pkl'), 'rb') as f:\n",
    "    pitcher_embedding_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o32Z70wnC3pA"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt6VBV-QruKm"
   },
   "source": [
    "#### Recategorize previous outcome to smaller class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1639250317179,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "0acRmSijrveg"
   },
   "outputs": [],
   "source": [
    "train_data['prev_outcome'] = np.select(\n",
    "    [\n",
    "      train_data['prev_description'].isin([\"ball\", \"blocked_ball\"]), \n",
    "      train_data['prev_description'].isin([\"foul\", \"foul_tip\"]),\n",
    "      (train_data['prev_description'] == \"hit_into_play\") & ~train_data['prev_events'].isin([\"single\", \"double\", \"triple\", \"home_run\"]),\n",
    "      (train_data['prev_description'] == \"hit_into_play\") & ~train_data['prev_events'].isin([\"single\", \"double\", \"triple\", \"home_run\"]),\n",
    "      train_data['prev_description'] == \"called_strike\",\n",
    "      train_data['prev_description'].isin([\"swinging_strike\", \"swinging_strike_blocked\"])\n",
    "    ], \n",
    "    [\n",
    "      'ball', \n",
    "      'foul',\n",
    "      'out',\n",
    "      'hit',\n",
    "      'called_strike',\n",
    "      'swinging_strike'\n",
    "    ], \n",
    "    default='other'\n",
    ")\n",
    "\n",
    "validation_data['prev_outcome'] = np.select(\n",
    "    [\n",
    "      validation_data['prev_description'].isin([\"ball\", \"blocked_ball\"]), \n",
    "      validation_data['prev_description'].isin([\"foul\", \"foul_tip\"]),\n",
    "      (validation_data['prev_description'] == \"hit_into_play\") & ~validation_data['prev_events'].isin([\"single\", \"double\", \"triple\", \"home_run\"]),\n",
    "      (validation_data['prev_description'] == \"hit_into_play\") & ~validation_data['prev_events'].isin([\"single\", \"double\", \"triple\", \"home_run\"]),\n",
    "      validation_data['prev_description'] == \"called_strike\",\n",
    "      validation_data['prev_description'].isin([\"swinging_strike\", \"swinging_strike_blocked\"])\n",
    "    ], \n",
    "    [\n",
    "      'ball', \n",
    "      'foul',\n",
    "      'out',\n",
    "      'hit',\n",
    "      'called_strike',\n",
    "      'swinging_strike'\n",
    "    ], \n",
    "    default='other'\n",
    ")\n",
    "\n",
    "test_data['prev_outcome'] = np.select(\n",
    "    [\n",
    "      test_data['prev_description'].isin([\"ball\", \"blocked_ball\"]), \n",
    "      test_data['prev_description'].isin([\"foul\", \"foul_tip\"]),\n",
    "      (test_data['prev_description'] == \"hit_into_play\") & ~test_data['prev_events'].isin([\"single\", \"double\", \"triple\", \"home_run\"]),\n",
    "      (test_data['prev_description'] == \"hit_into_play\") & ~test_data['prev_events'].isin([\"single\", \"double\", \"triple\", \"home_run\"]),\n",
    "      test_data['prev_description'] == \"called_strike\",\n",
    "      test_data['prev_description'].isin([\"swinging_strike\", \"swinging_strike_blocked\"])\n",
    "    ], \n",
    "    [\n",
    "      'ball', \n",
    "      'foul',\n",
    "      'out',\n",
    "      'hit',\n",
    "      'called_strike',\n",
    "      'swinging_strike'\n",
    "    ], \n",
    "    default='other'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kqPzdu-Of-A"
   },
   "source": [
    "### Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRZGufelLuPe"
   },
   "source": [
    "The batters and pitchers will be passed in as embedding vectors into the model, which is a lower dimensional representation. We will join the batter and pitcher columns from the original data to the embedding vectors we loaded in earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1639250317179,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "oaq5Fd3Ckdqp"
   },
   "outputs": [],
   "source": [
    "# get the list of the batter column names from the embedding data frame\n",
    "batter_cols = list(['batter_' + str(i) for i in range(9)]) # these are 9 dimensional embeddings\n",
    "batter_cols.append('batter')\n",
    "\n",
    "# get the list of the pitcher column names from the embedding data frame\n",
    "pitcher_cols = list(['pitcher_' + str(i) for i in range(9)]) # these are 9 dimensional embeddings\n",
    "pitcher_cols.append('pitcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2767,
     "status": "ok",
     "timestamp": 1639250319943,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "9XwtBZP0FnDa",
    "outputId": "70e1d0b8-1d14-4755-d8ec-277d8888374a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##### Batter Embedding Integration ----\n",
    "\n",
    "## Train --\n",
    "\n",
    "# get the existing statcast data row count\n",
    "train_data_nrow = len(train_data)\n",
    "\n",
    "# join the batter embeddings to the data\n",
    "train_data = train_data.merge(batter_embedding_df[batter_cols], on='batter', how='left')\n",
    "\n",
    "# get the new statcast data row count\n",
    "train_data_batter_nrow = len(train_data)\n",
    "\n",
    "# make sure the join doesn't produce duplicate records\n",
    "print(train_data_nrow == train_data_batter_nrow)\n",
    "\n",
    "## Validation --\n",
    "\n",
    "# get the existing statcast data row count\n",
    "validation_data_nrow = len(validation_data)\n",
    "\n",
    "# join the batter embeddings to the data\n",
    "validation_data = validation_data.merge(batter_embedding_df[batter_cols], on='batter', how='left')\n",
    "\n",
    "# get the new statcast data row count\n",
    "validation_data_batter_nrow = len(validation_data)\n",
    "\n",
    "# make sure the join doesn't produce duplicate records\n",
    "print(validation_data_nrow == validation_data_batter_nrow)\n",
    "\n",
    "## Test --\n",
    "\n",
    "# get the existing statcast data row count\n",
    "test_data_nrow = len(test_data)\n",
    "\n",
    "# join the batter embeddings to the data\n",
    "test_data = test_data.merge(batter_embedding_df[batter_cols], on='batter', how='left')\n",
    "\n",
    "# get the new statcast data row count\n",
    "test_data_batter_nrow = len(test_data)\n",
    "\n",
    "# make sure the join doesn't produce duplicate records\n",
    "print(test_data_nrow == test_data_batter_nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1639250320720,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "wr86EtN6HAqt",
    "outputId": "48d44377-746b-4c2a-d67b-352840265647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##### Pitcher Embedding Integration ----\n",
    "\n",
    "## Train --\n",
    "\n",
    "# get the existing statcast data row count\n",
    "train_data_nrow = len(train_data)\n",
    "\n",
    "# join the pitcher embeddings to the data\n",
    "train_data = train_data.merge(pitcher_embedding_df[pitcher_cols], on='pitcher', how='left')\n",
    "\n",
    "# get the new statcast data row count\n",
    "train_data_pitcher_nrow = len(train_data)\n",
    "\n",
    "# make sure the join doesn't produce duplicate records\n",
    "print(train_data_nrow == train_data_pitcher_nrow)\n",
    "\n",
    "## Validation --\n",
    "\n",
    "# get the existing statcast data row count\n",
    "validation_data_nrow = len(validation_data)\n",
    "\n",
    "# join the pitcher embeddings to the data\n",
    "validation_data = validation_data.merge(pitcher_embedding_df[pitcher_cols], on='pitcher', how='left')\n",
    "\n",
    "# get the new statcast data row count\n",
    "validation_data_pitcher_nrow = len(validation_data)\n",
    "\n",
    "# make sure the join doesn't produce duplicate records\n",
    "print(validation_data_nrow == validation_data_pitcher_nrow)\n",
    "\n",
    "## Test --\n",
    "\n",
    "# get the existing statcast data row count\n",
    "test_data_nrow = len(test_data)\n",
    "\n",
    "# join the pitcher embeddings to the data\n",
    "test_data = test_data.merge(pitcher_embedding_df[pitcher_cols], on='pitcher', how='left')\n",
    "\n",
    "# get the new statcast data row count\n",
    "test_data_pitcher_nrow = len(test_data)\n",
    "\n",
    "# make sure the join doesn't produce duplicate records\n",
    "print(test_data_nrow == test_data_pitcher_nrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-WroBFzMKZU"
   },
   "source": [
    "After the join, we need to remove features that are not needed for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1639250322081,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "2WucUOfD3XTQ"
   },
   "outputs": [],
   "source": [
    "# drop columns from data set we do not care about\n",
    "train_data = train_data.drop(columns=['batter', 'pitcher', 'prev_game_pitch_number', 'prev_game_pitch_number', 'prev_game_pk', 'prev_pitch_number', 'prev_at_bat_number', 'game_pitch_number'])\n",
    "validation_data = validation_data.drop(columns=['batter', 'pitcher', 'prev_game_pitch_number', 'prev_game_pitch_number', 'prev_game_pk', 'prev_pitch_number', 'prev_at_bat_number', 'game_pitch_number'])\n",
    "test_data = test_data.drop(columns=['batter', 'pitcher', 'prev_game_pitch_number', 'prev_game_pitch_number', 'prev_game_pk', 'prev_pitch_number', 'prev_at_bat_number', 'game_pitch_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1639250322376,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "AqEt6C4wEOv9"
   },
   "outputs": [],
   "source": [
    "# assign the input and response features\n",
    "x_train = train_data.drop(['pitch_class'], axis = 1) # features\n",
    "y_train = train_data[[\"game_pk\", \"at_bat_number\", 'pitch_class']] # label\n",
    "\n",
    "x_validation = validation_data.drop(['pitch_class'], axis = 1) # features\n",
    "y_validation = validation_data[[\"game_pk\", \"at_bat_number\", 'pitch_class']] # label\n",
    "\n",
    "x_test = test_data.drop(['pitch_class'], axis = 1) # features\n",
    "y_test = test_data[[\"game_pk\", \"at_bat_number\", 'pitch_class']] # label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMr2O7blOKxb"
   },
   "source": [
    "After checking for missing data after these transformations, we see there are still some of the previous pitch data features which are missing. These represent the first pitches of the game and so there is no previous pitch information. These features will be imputed with a 0 when passed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "executionInfo": {
     "elapsed": 1664,
     "status": "ok",
     "timestamp": 1639250324037,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "m2UaRx2VoiHu",
    "outputId": "5de8882d-3ea2-4036-f254-b814c0313be9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent</th>\n",
       "      <th>Train Missing Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prev_hc_x</th>\n",
       "      <td>82.95000</td>\n",
       "      <td>651819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_hc_y</th>\n",
       "      <td>82.95000</td>\n",
       "      <td>651819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_launch_speed_angle</th>\n",
       "      <td>82.89000</td>\n",
       "      <td>651364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_bb_type</th>\n",
       "      <td>82.68000</td>\n",
       "      <td>649651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_events</th>\n",
       "      <td>74.83000</td>\n",
       "      <td>587991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_hit_distance_sc</th>\n",
       "      <td>73.70000</td>\n",
       "      <td>579104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_release_spin_rate</th>\n",
       "      <td>1.97000</td>\n",
       "      <td>15500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_effective_speed</th>\n",
       "      <td>0.66000</td>\n",
       "      <td>5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_release_extension</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_release_speed</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_release_pos_x</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_plate_x</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_zone</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_spin_axis</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_release_pos_y</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_release_pos_z</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_pfx_z</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_plate_z</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_az</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_ax</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_vz0</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_vy0</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_vx0</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_pfx_x</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_ay</th>\n",
       "      <td>0.48000</td>\n",
       "      <td>3786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_description</th>\n",
       "      <td>0.39000</td>\n",
       "      <td>3076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_pitch_class</th>\n",
       "      <td>0.39000</td>\n",
       "      <td>3076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_type</th>\n",
       "      <td>0.39000</td>\n",
       "      <td>3076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Percent  Train Missing Count\n",
       "prev_hc_x               82.95000               651819\n",
       "prev_hc_y               82.95000               651819\n",
       "prev_launch_speed_angle 82.89000               651364\n",
       "prev_bb_type            82.68000               649651\n",
       "prev_events             74.83000               587991\n",
       "prev_hit_distance_sc    73.70000               579104\n",
       "prev_release_spin_rate   1.97000                15500\n",
       "prev_effective_speed     0.66000                 5196\n",
       "prev_release_extension   0.48000                 3788\n",
       "prev_release_speed       0.48000                 3788\n",
       "prev_release_pos_x       0.48000                 3786\n",
       "prev_plate_x             0.48000                 3786\n",
       "prev_zone                0.48000                 3786\n",
       "prev_spin_axis           0.48000                 3786\n",
       "prev_release_pos_y       0.48000                 3786\n",
       "prev_release_pos_z       0.48000                 3786\n",
       "prev_pfx_z               0.48000                 3786\n",
       "prev_plate_z             0.48000                 3786\n",
       "prev_az                  0.48000                 3786\n",
       "prev_ax                  0.48000                 3786\n",
       "prev_vz0                 0.48000                 3786\n",
       "prev_vy0                 0.48000                 3786\n",
       "prev_vx0                 0.48000                 3786\n",
       "prev_pfx_x               0.48000                 3786\n",
       "prev_ay                  0.48000                 3786\n",
       "prev_description         0.39000                 3076\n",
       "prev_pitch_class         0.39000                 3076\n",
       "prev_type                0.39000                 3076"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Missing Data Check ----\n",
    "\n",
    "# game state data\n",
    "percent = (x_train.isnull().sum()/x_train.isnull().count()*100).sort_values(ascending = False).round(2)\n",
    "sum_missing = x_train.isna().sum().sort_values(ascending = False)\n",
    "missing_data  = pd.concat([percent, sum_missing], axis=1, keys=['Percent', \"Train Missing Count\"])\n",
    "display(missing_data[missing_data['Percent'] != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3Q2_51POrGF"
   },
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvmEM6pNOtKO"
   },
   "source": [
    "To prepare the inputs and targets, we will first pass the input data into a preparation pipeline. The pipeline will impute missing categorical features with \"Unknown\" and one hot encode the features. All numeric features missing will be imputed with 0 and then scaled. We start by setting the features we will pass into the pipeline. The ID features are still needed so we can split the data by at bat and eventually pass that in as a sequence to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1639250324038,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "29fbxqOIAFg-"
   },
   "outputs": [],
   "source": [
    "# id features\n",
    "id_features = [\"game_pk\", \"at_bat_number\"]\n",
    "\n",
    "# ohe and no imputation\n",
    "categorical_features = ['outs_when_up',\n",
    "                        'count',\n",
    "                        'stand',\n",
    "                        'p_throws',\n",
    "                        'prev_pitch_class',\n",
    "                        'prev_outcome',\n",
    "                        'tto']\n",
    "\n",
    "# numeric and impute with 0\n",
    "numerical_features = ['recent_fourseam_usage',\n",
    "                      'recent_twoseam_usage',\n",
    "                      'recent_cutter_usage',\n",
    "                      'recent_slider_usage',\n",
    "                      'recent_curveball_usage',\n",
    "                      'recent_changeup_usage',\n",
    "                      'long_term_fourseam_usage',\n",
    "                      'long_term_twoseam_usage',\n",
    "                      'long_term_cutter_usage',\n",
    "                      'long_term_slider_usage',\n",
    "                      'long_term_curveball_usage',\n",
    "                      'long_term_changeup_usage',\n",
    "                      'Changeup_release_speed_m',\n",
    "                      'Curveball_release_speed_m',\n",
    "                      'Cutter_release_speed_m',\n",
    "                      'Fourseam_release_speed_m',\n",
    "                      'Slider_release_speed_m',\n",
    "                      'Twoseam_release_speed_m',\n",
    "                      'Changeup_release_spin_rate_m',\n",
    "                      'Curveball_release_spin_rate_m',\n",
    "                      'Cutter_release_spin_rate_m',\n",
    "                      'Fourseam_release_spin_rate_m',\n",
    "                      'Slider_release_spin_rate_m',\n",
    "                      'Twoseam_release_spin_rate_m',\n",
    "                      'Changeup_strike_per_m',\n",
    "                      'Curveball_strike_per_m',\n",
    "                      'Cutter_strike_per_m',\n",
    "                      'Fourseam_strike_per_m',\n",
    "                      'Slider_strike_per_m',\n",
    "                      'Twoseam_strike_per_m',\n",
    "                      'Changeup_whiff_per_m',\n",
    "                      'Curveball_whiff_per_m',\n",
    "                      'Cutter_whiff_per_m',\n",
    "                      'Fourseam_whiff_per_m',\n",
    "                      'Slider_whiff_per_m',\n",
    "                      'Twoseam_whiff_per_m',\n",
    "                      'Changeup_woba_value_m',\n",
    "                      'Curveball_woba_value_m',\n",
    "                      'Cutter_woba_value_m',\n",
    "                      'Fourseam_woba_value_m',\n",
    "                      'Slider_woba_value_m',\n",
    "                      'Twoseam_woba_value_m',\n",
    "                      'batter_Changeup_whiff_per_m',\n",
    "                      'batter_Curveball_whiff_per_m',\n",
    "                      'batter_Cutter_whiff_per_m',\n",
    "                      'batter_Fourseam_whiff_per_m',\n",
    "                      'batter_Slider_whiff_per_m',\n",
    "                      'batter_Twoseam_whiff_per_m',\n",
    "                      'batter_Changeup_woba_value_m',\n",
    "                      'batter_Curveball_woba_value_m',\n",
    "                      'batter_Cutter_woba_value_m',\n",
    "                      'batter_Fourseam_woba_value_m',\n",
    "                      'batter_Slider_woba_value_m',\n",
    "                      'batter_Twoseam_woba_value_m',\n",
    "                      'bat_score',\n",
    "                      'fld_score',\n",
    "                      'on_1b',\n",
    "                      'on_2b',\n",
    "                      'on_3b',\n",
    "                      'pitch_number',\n",
    "                      'pitcher_pitch_number',\n",
    "                      'prev_release_speed',\n",
    "                      'prev_release_spin_rate',\n",
    "                      'prev_plate_x',\n",
    "                      'prev_plate_z',\n",
    "                      'prev_hit_distance_sc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwx_gSKgPgaB"
   },
   "source": [
    "We specify the pipeline below to perform the data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1639250324039,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "oIqsH4af_Ab0"
   },
   "outputs": [],
   "source": [
    "##### Data Pipelines ----\n",
    "\n",
    "# create pipeline for numeric features\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=0.0)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# create pipeline for numeric features\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=\"NA\")),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', drop='if_binary')),\n",
    "    ])\n",
    "\n",
    "# specify the column transformer for numeric features\n",
    "data_pipeline = ColumnTransformer(\n",
    "    [(\"num_pipeline\", num_pipeline, numerical_features),\n",
    "     (\"cat_pipeline\", cat_pipeline, categorical_features)],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcfCa_T-PnYE"
   },
   "source": [
    "We will then apply the data transformation pipeline to the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 9634,
     "status": "ok",
     "timestamp": 1639250333669,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "eFnePUAnLjfg"
   },
   "outputs": [],
   "source": [
    "# convert categorical variables to strings\n",
    "x_train[categorical_features] = x_train[categorical_features].astype(str) \n",
    "\n",
    "# apply the pipeline to the input data\n",
    "x_train_transformed = data_pipeline.fit_transform(x_train[id_features + numerical_features + categorical_features])\n",
    "\n",
    "# display the results of the transformation\n",
    "x_train_transformed = pd.DataFrame(x_train_transformed, columns= numerical_features + list(data_pipeline.transformers_[1][1].named_steps['ohe'].get_feature_names(categorical_features)) + id_features)\n",
    "\n",
    "# convert categorical variables to strings\n",
    "x_validation[categorical_features] = x_validation[categorical_features].astype(str) \n",
    "\n",
    "# apply the pipeline to the validation data\n",
    "x_validation_transformed = data_pipeline.transform(x_validation[id_features + numerical_features + categorical_features])\n",
    "\n",
    "# display the results of the transformation\n",
    "x_validation_transformed = pd.DataFrame(x_validation_transformed, columns= numerical_features + list(data_pipeline.transformers_[1][1].named_steps['ohe'].get_feature_names(categorical_features)) + id_features)\n",
    "\n",
    "# convert categorical variables to strings\n",
    "x_test[categorical_features] = x_test[categorical_features].astype(str) \n",
    "\n",
    "# apply the pipeline to the validation data\n",
    "x_test_transformed = data_pipeline.transform(x_test[id_features + numerical_features + categorical_features])\n",
    "\n",
    "# display the results of the transformation\n",
    "x_test_transformed = pd.DataFrame(x_test_transformed, columns= numerical_features + list(data_pipeline.transformers_[1][1].named_steps['ohe'].get_feature_names(categorical_features)) + id_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1639250333670,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "mKgSkqtROVvx",
    "outputId": "28424ae9-4060-443d-94bc-dd8a15ff6595"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recent_fourseam_usage</th>\n",
       "      <th>recent_twoseam_usage</th>\n",
       "      <th>recent_cutter_usage</th>\n",
       "      <th>recent_slider_usage</th>\n",
       "      <th>recent_curveball_usage</th>\n",
       "      <th>recent_changeup_usage</th>\n",
       "      <th>long_term_fourseam_usage</th>\n",
       "      <th>long_term_twoseam_usage</th>\n",
       "      <th>long_term_cutter_usage</th>\n",
       "      <th>long_term_slider_usage</th>\n",
       "      <th>long_term_curveball_usage</th>\n",
       "      <th>long_term_changeup_usage</th>\n",
       "      <th>Changeup_release_speed_m</th>\n",
       "      <th>Curveball_release_speed_m</th>\n",
       "      <th>Cutter_release_speed_m</th>\n",
       "      <th>Fourseam_release_speed_m</th>\n",
       "      <th>Slider_release_speed_m</th>\n",
       "      <th>Twoseam_release_speed_m</th>\n",
       "      <th>Changeup_release_spin_rate_m</th>\n",
       "      <th>Curveball_release_spin_rate_m</th>\n",
       "      <th>Cutter_release_spin_rate_m</th>\n",
       "      <th>Fourseam_release_spin_rate_m</th>\n",
       "      <th>Slider_release_spin_rate_m</th>\n",
       "      <th>Twoseam_release_spin_rate_m</th>\n",
       "      <th>Changeup_strike_per_m</th>\n",
       "      <th>Curveball_strike_per_m</th>\n",
       "      <th>Cutter_strike_per_m</th>\n",
       "      <th>Fourseam_strike_per_m</th>\n",
       "      <th>Slider_strike_per_m</th>\n",
       "      <th>Twoseam_strike_per_m</th>\n",
       "      <th>Changeup_whiff_per_m</th>\n",
       "      <th>Curveball_whiff_per_m</th>\n",
       "      <th>Cutter_whiff_per_m</th>\n",
       "      <th>Fourseam_whiff_per_m</th>\n",
       "      <th>Slider_whiff_per_m</th>\n",
       "      <th>Twoseam_whiff_per_m</th>\n",
       "      <th>Changeup_woba_value_m</th>\n",
       "      <th>Curveball_woba_value_m</th>\n",
       "      <th>Cutter_woba_value_m</th>\n",
       "      <th>Fourseam_woba_value_m</th>\n",
       "      <th>Slider_woba_value_m</th>\n",
       "      <th>Twoseam_woba_value_m</th>\n",
       "      <th>batter_Changeup_whiff_per_m</th>\n",
       "      <th>batter_Curveball_whiff_per_m</th>\n",
       "      <th>batter_Cutter_whiff_per_m</th>\n",
       "      <th>batter_Fourseam_whiff_per_m</th>\n",
       "      <th>batter_Slider_whiff_per_m</th>\n",
       "      <th>batter_Twoseam_whiff_per_m</th>\n",
       "      <th>batter_Changeup_woba_value_m</th>\n",
       "      <th>batter_Curveball_woba_value_m</th>\n",
       "      <th>batter_Cutter_woba_value_m</th>\n",
       "      <th>batter_Fourseam_woba_value_m</th>\n",
       "      <th>batter_Slider_woba_value_m</th>\n",
       "      <th>batter_Twoseam_woba_value_m</th>\n",
       "      <th>bat_score</th>\n",
       "      <th>fld_score</th>\n",
       "      <th>on_1b</th>\n",
       "      <th>on_2b</th>\n",
       "      <th>on_3b</th>\n",
       "      <th>pitch_number</th>\n",
       "      <th>pitcher_pitch_number</th>\n",
       "      <th>prev_release_speed</th>\n",
       "      <th>prev_release_spin_rate</th>\n",
       "      <th>prev_plate_x</th>\n",
       "      <th>prev_plate_z</th>\n",
       "      <th>prev_hit_distance_sc</th>\n",
       "      <th>outs_when_up_0.0</th>\n",
       "      <th>outs_when_up_1.0</th>\n",
       "      <th>outs_when_up_2.0</th>\n",
       "      <th>count_0-0</th>\n",
       "      <th>count_0-1</th>\n",
       "      <th>count_0-2</th>\n",
       "      <th>count_1-0</th>\n",
       "      <th>count_1-1</th>\n",
       "      <th>count_1-2</th>\n",
       "      <th>count_2-0</th>\n",
       "      <th>count_2-1</th>\n",
       "      <th>count_2-2</th>\n",
       "      <th>count_3-0</th>\n",
       "      <th>count_3-1</th>\n",
       "      <th>count_3-2</th>\n",
       "      <th>stand_R</th>\n",
       "      <th>p_throws_R</th>\n",
       "      <th>prev_pitch_class_changeup</th>\n",
       "      <th>prev_pitch_class_curveball</th>\n",
       "      <th>prev_pitch_class_cutter</th>\n",
       "      <th>prev_pitch_class_fourseam</th>\n",
       "      <th>prev_pitch_class_nan</th>\n",
       "      <th>prev_pitch_class_slider</th>\n",
       "      <th>prev_pitch_class_twoseam</th>\n",
       "      <th>prev_outcome_ball</th>\n",
       "      <th>prev_outcome_called_strike</th>\n",
       "      <th>prev_outcome_foul</th>\n",
       "      <th>prev_outcome_other</th>\n",
       "      <th>prev_outcome_out</th>\n",
       "      <th>prev_outcome_swinging_strike</th>\n",
       "      <th>tto_1.0</th>\n",
       "      <th>tto_2.0</th>\n",
       "      <th>tto_3.0</th>\n",
       "      <th>tto_4.0</th>\n",
       "      <th>tto_5.0</th>\n",
       "      <th>game_pk</th>\n",
       "      <th>at_bat_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.78043</td>\n",
       "      <td>0.58270</td>\n",
       "      <td>-0.44229</td>\n",
       "      <td>-1.03107</td>\n",
       "      <td>1.25860</td>\n",
       "      <td>0.97975</td>\n",
       "      <td>-0.91678</td>\n",
       "      <td>0.30666</td>\n",
       "      <td>-0.45978</td>\n",
       "      <td>-1.11133</td>\n",
       "      <td>2.55008</td>\n",
       "      <td>0.58950</td>\n",
       "      <td>0.36672</td>\n",
       "      <td>0.75330</td>\n",
       "      <td>-0.80604</td>\n",
       "      <td>0.29085</td>\n",
       "      <td>-2.19225</td>\n",
       "      <td>0.45772</td>\n",
       "      <td>0.76729</td>\n",
       "      <td>0.26517</td>\n",
       "      <td>-0.80229</td>\n",
       "      <td>-0.15407</td>\n",
       "      <td>-2.12834</td>\n",
       "      <td>0.11610</td>\n",
       "      <td>1.01428</td>\n",
       "      <td>1.01829</td>\n",
       "      <td>-0.74962</td>\n",
       "      <td>-3.94565</td>\n",
       "      <td>-1.92348</td>\n",
       "      <td>0.57852</td>\n",
       "      <td>-0.00953</td>\n",
       "      <td>1.02244</td>\n",
       "      <td>-0.53746</td>\n",
       "      <td>-1.24991</td>\n",
       "      <td>-1.27481</td>\n",
       "      <td>0.35641</td>\n",
       "      <td>0.95449</td>\n",
       "      <td>-0.29283</td>\n",
       "      <td>-0.54859</td>\n",
       "      <td>-1.57796</td>\n",
       "      <td>-1.03809</td>\n",
       "      <td>0.03741</td>\n",
       "      <td>4.60990</td>\n",
       "      <td>0.39562</td>\n",
       "      <td>-0.37305</td>\n",
       "      <td>-1.42516</td>\n",
       "      <td>-1.93447</td>\n",
       "      <td>-1.20335</td>\n",
       "      <td>-1.56294</td>\n",
       "      <td>-0.21872</td>\n",
       "      <td>0.25102</td>\n",
       "      <td>-1.96208</td>\n",
       "      <td>2.23426</td>\n",
       "      <td>-2.02054</td>\n",
       "      <td>-0.84708</td>\n",
       "      <td>-0.85320</td>\n",
       "      <td>-0.65517</td>\n",
       "      <td>-0.46297</td>\n",
       "      <td>-0.31102</td>\n",
       "      <td>-1.10105</td>\n",
       "      <td>-1.17847</td>\n",
       "      <td>-10.37162</td>\n",
       "      <td>-4.97929</td>\n",
       "      <td>-0.05153</td>\n",
       "      <td>-2.34281</td>\n",
       "      <td>-0.45101</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>490099.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.78043</td>\n",
       "      <td>0.58270</td>\n",
       "      <td>-0.44229</td>\n",
       "      <td>-1.03107</td>\n",
       "      <td>1.25860</td>\n",
       "      <td>0.97975</td>\n",
       "      <td>-0.91678</td>\n",
       "      <td>0.30666</td>\n",
       "      <td>-0.45978</td>\n",
       "      <td>-1.11133</td>\n",
       "      <td>2.55008</td>\n",
       "      <td>0.58950</td>\n",
       "      <td>0.36672</td>\n",
       "      <td>0.75330</td>\n",
       "      <td>-0.80604</td>\n",
       "      <td>0.29085</td>\n",
       "      <td>-2.19225</td>\n",
       "      <td>0.46121</td>\n",
       "      <td>0.76729</td>\n",
       "      <td>0.26517</td>\n",
       "      <td>-0.80229</td>\n",
       "      <td>-0.15407</td>\n",
       "      <td>-2.12834</td>\n",
       "      <td>0.29116</td>\n",
       "      <td>1.01428</td>\n",
       "      <td>1.01829</td>\n",
       "      <td>-0.74962</td>\n",
       "      <td>-3.94565</td>\n",
       "      <td>-1.92348</td>\n",
       "      <td>1.83526</td>\n",
       "      <td>-0.00953</td>\n",
       "      <td>1.02244</td>\n",
       "      <td>-0.53746</td>\n",
       "      <td>-1.24991</td>\n",
       "      <td>-1.27481</td>\n",
       "      <td>-0.81058</td>\n",
       "      <td>0.95449</td>\n",
       "      <td>-0.29283</td>\n",
       "      <td>-0.54859</td>\n",
       "      <td>-1.57796</td>\n",
       "      <td>-1.03809</td>\n",
       "      <td>-1.27761</td>\n",
       "      <td>4.60990</td>\n",
       "      <td>0.39562</td>\n",
       "      <td>-0.37305</td>\n",
       "      <td>-1.42516</td>\n",
       "      <td>-1.93447</td>\n",
       "      <td>-1.20335</td>\n",
       "      <td>-1.56294</td>\n",
       "      <td>-0.21872</td>\n",
       "      <td>0.25102</td>\n",
       "      <td>-1.96208</td>\n",
       "      <td>2.23426</td>\n",
       "      <td>-2.02054</td>\n",
       "      <td>-0.84708</td>\n",
       "      <td>-0.85320</td>\n",
       "      <td>-0.65517</td>\n",
       "      <td>-0.46297</td>\n",
       "      <td>-0.31102</td>\n",
       "      <td>-0.52608</td>\n",
       "      <td>-1.14384</td>\n",
       "      <td>0.78082</td>\n",
       "      <td>-0.08263</td>\n",
       "      <td>0.27353</td>\n",
       "      <td>2.19093</td>\n",
       "      <td>-0.45101</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>490099.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.92706</td>\n",
       "      <td>0.73618</td>\n",
       "      <td>-0.44229</td>\n",
       "      <td>-1.03107</td>\n",
       "      <td>1.25860</td>\n",
       "      <td>0.97975</td>\n",
       "      <td>-0.91678</td>\n",
       "      <td>0.35530</td>\n",
       "      <td>-0.45978</td>\n",
       "      <td>-1.11133</td>\n",
       "      <td>2.46201</td>\n",
       "      <td>0.58950</td>\n",
       "      <td>0.36672</td>\n",
       "      <td>0.75330</td>\n",
       "      <td>-0.80604</td>\n",
       "      <td>0.29085</td>\n",
       "      <td>-2.19225</td>\n",
       "      <td>0.48617</td>\n",
       "      <td>0.76729</td>\n",
       "      <td>0.26517</td>\n",
       "      <td>-0.80229</td>\n",
       "      <td>-0.15407</td>\n",
       "      <td>-2.12834</td>\n",
       "      <td>0.19522</td>\n",
       "      <td>1.01428</td>\n",
       "      <td>1.01829</td>\n",
       "      <td>-0.74962</td>\n",
       "      <td>-3.94565</td>\n",
       "      <td>-1.92348</td>\n",
       "      <td>-0.25931</td>\n",
       "      <td>-0.00953</td>\n",
       "      <td>1.02244</td>\n",
       "      <td>-0.53746</td>\n",
       "      <td>-1.24991</td>\n",
       "      <td>-1.27481</td>\n",
       "      <td>-0.81058</td>\n",
       "      <td>0.95449</td>\n",
       "      <td>-0.29283</td>\n",
       "      <td>-0.54859</td>\n",
       "      <td>-1.57796</td>\n",
       "      <td>-1.03809</td>\n",
       "      <td>-1.27761</td>\n",
       "      <td>4.60990</td>\n",
       "      <td>0.39562</td>\n",
       "      <td>-0.37305</td>\n",
       "      <td>-1.42516</td>\n",
       "      <td>-1.93447</td>\n",
       "      <td>-1.20335</td>\n",
       "      <td>-1.56294</td>\n",
       "      <td>-0.21872</td>\n",
       "      <td>0.25102</td>\n",
       "      <td>-1.96208</td>\n",
       "      <td>2.23426</td>\n",
       "      <td>-2.02054</td>\n",
       "      <td>-0.84708</td>\n",
       "      <td>-0.85320</td>\n",
       "      <td>-0.65517</td>\n",
       "      <td>-0.46297</td>\n",
       "      <td>-0.31102</td>\n",
       "      <td>0.04889</td>\n",
       "      <td>-1.10922</td>\n",
       "      <td>0.89834</td>\n",
       "      <td>-0.08719</td>\n",
       "      <td>-0.72488</td>\n",
       "      <td>0.08636</td>\n",
       "      <td>-0.45101</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>490099.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.92706</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>-0.44229</td>\n",
       "      <td>-1.03107</td>\n",
       "      <td>0.98958</td>\n",
       "      <td>0.97975</td>\n",
       "      <td>-0.91678</td>\n",
       "      <td>0.40394</td>\n",
       "      <td>-0.45978</td>\n",
       "      <td>-1.11133</td>\n",
       "      <td>2.37394</td>\n",
       "      <td>0.58950</td>\n",
       "      <td>0.36672</td>\n",
       "      <td>0.75330</td>\n",
       "      <td>-0.80604</td>\n",
       "      <td>0.29085</td>\n",
       "      <td>0.45495</td>\n",
       "      <td>0.48617</td>\n",
       "      <td>0.76729</td>\n",
       "      <td>0.26517</td>\n",
       "      <td>-0.80229</td>\n",
       "      <td>-0.15407</td>\n",
       "      <td>0.26505</td>\n",
       "      <td>0.19522</td>\n",
       "      <td>1.01428</td>\n",
       "      <td>1.01829</td>\n",
       "      <td>-0.74962</td>\n",
       "      <td>-3.94565</td>\n",
       "      <td>1.96877</td>\n",
       "      <td>-0.25931</td>\n",
       "      <td>-0.00953</td>\n",
       "      <td>1.02244</td>\n",
       "      <td>-0.53746</td>\n",
       "      <td>-1.24991</td>\n",
       "      <td>-1.27481</td>\n",
       "      <td>-0.81058</td>\n",
       "      <td>0.95449</td>\n",
       "      <td>-0.29283</td>\n",
       "      <td>-0.54859</td>\n",
       "      <td>-1.57796</td>\n",
       "      <td>2.98801</td>\n",
       "      <td>-1.27761</td>\n",
       "      <td>4.60990</td>\n",
       "      <td>0.39562</td>\n",
       "      <td>-0.37305</td>\n",
       "      <td>-1.42516</td>\n",
       "      <td>-1.93447</td>\n",
       "      <td>-1.20335</td>\n",
       "      <td>-1.56294</td>\n",
       "      <td>-0.21872</td>\n",
       "      <td>0.25102</td>\n",
       "      <td>-1.96208</td>\n",
       "      <td>3.30663</td>\n",
       "      <td>-2.02054</td>\n",
       "      <td>-0.84708</td>\n",
       "      <td>-0.85320</td>\n",
       "      <td>-0.65517</td>\n",
       "      <td>-0.46297</td>\n",
       "      <td>-0.31102</td>\n",
       "      <td>0.62385</td>\n",
       "      <td>-1.07459</td>\n",
       "      <td>1.06287</td>\n",
       "      <td>-0.38154</td>\n",
       "      <td>-0.06314</td>\n",
       "      <td>-0.95023</td>\n",
       "      <td>-0.45101</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>490099.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.07369</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>-0.44229</td>\n",
       "      <td>-0.82157</td>\n",
       "      <td>0.98958</td>\n",
       "      <td>0.97975</td>\n",
       "      <td>-0.91678</td>\n",
       "      <td>0.40394</td>\n",
       "      <td>-0.45978</td>\n",
       "      <td>-1.04349</td>\n",
       "      <td>2.37394</td>\n",
       "      <td>0.49306</td>\n",
       "      <td>0.36672</td>\n",
       "      <td>0.75330</td>\n",
       "      <td>-0.80604</td>\n",
       "      <td>0.29085</td>\n",
       "      <td>0.45495</td>\n",
       "      <td>0.50756</td>\n",
       "      <td>0.76729</td>\n",
       "      <td>0.26517</td>\n",
       "      <td>-0.80229</td>\n",
       "      <td>-0.15407</td>\n",
       "      <td>0.26505</td>\n",
       "      <td>0.18853</td>\n",
       "      <td>1.01428</td>\n",
       "      <td>1.01829</td>\n",
       "      <td>-0.74962</td>\n",
       "      <td>-3.94565</td>\n",
       "      <td>1.96877</td>\n",
       "      <td>0.43888</td>\n",
       "      <td>-0.00953</td>\n",
       "      <td>1.02244</td>\n",
       "      <td>-0.53746</td>\n",
       "      <td>-1.24991</td>\n",
       "      <td>-1.27481</td>\n",
       "      <td>-0.81058</td>\n",
       "      <td>0.95449</td>\n",
       "      <td>-0.29283</td>\n",
       "      <td>-0.54859</td>\n",
       "      <td>-1.57796</td>\n",
       "      <td>2.98801</td>\n",
       "      <td>-1.27761</td>\n",
       "      <td>0.63669</td>\n",
       "      <td>0.32120</td>\n",
       "      <td>-0.49321</td>\n",
       "      <td>-0.30970</td>\n",
       "      <td>1.63676</td>\n",
       "      <td>-1.20335</td>\n",
       "      <td>-0.59329</td>\n",
       "      <td>0.87771</td>\n",
       "      <td>0.55227</td>\n",
       "      <td>1.32536</td>\n",
       "      <td>-0.06686</td>\n",
       "      <td>-2.02054</td>\n",
       "      <td>-0.84708</td>\n",
       "      <td>-0.85320</td>\n",
       "      <td>1.52631</td>\n",
       "      <td>-0.46297</td>\n",
       "      <td>-0.31102</td>\n",
       "      <td>-1.10105</td>\n",
       "      <td>-1.03997</td>\n",
       "      <td>-0.42961</td>\n",
       "      <td>0.02690</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>-0.31152</td>\n",
       "      <td>2.23651</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>490099.00000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recent_fourseam_usage  recent_twoseam_usage  recent_cutter_usage  recent_slider_usage  recent_curveball_usage  recent_changeup_usage  long_term_fourseam_usage  long_term_twoseam_usage  long_term_cutter_usage  long_term_slider_usage  long_term_curveball_usage  long_term_changeup_usage  Changeup_release_speed_m  Curveball_release_speed_m  Cutter_release_speed_m  Fourseam_release_speed_m  Slider_release_speed_m  Twoseam_release_speed_m  Changeup_release_spin_rate_m  Curveball_release_spin_rate_m  Cutter_release_spin_rate_m  Fourseam_release_spin_rate_m  Slider_release_spin_rate_m  Twoseam_release_spin_rate_m  Changeup_strike_per_m  Curveball_strike_per_m  Cutter_strike_per_m  Fourseam_strike_per_m  Slider_strike_per_m  Twoseam_strike_per_m  Changeup_whiff_per_m  Curveball_whiff_per_m  Cutter_whiff_per_m  Fourseam_whiff_per_m  Slider_whiff_per_m  Twoseam_whiff_per_m  Changeup_woba_value_m  Curveball_woba_value_m  Cutter_woba_value_m  Fourseam_woba_value_m  Slider_woba_value_m  Twoseam_woba_value_m  batter_Changeup_whiff_per_m  batter_Curveball_whiff_per_m  batter_Cutter_whiff_per_m  batter_Fourseam_whiff_per_m  batter_Slider_whiff_per_m  batter_Twoseam_whiff_per_m  batter_Changeup_woba_value_m  batter_Curveball_woba_value_m  batter_Cutter_woba_value_m  batter_Fourseam_woba_value_m  batter_Slider_woba_value_m  batter_Twoseam_woba_value_m  bat_score  fld_score    on_1b    on_2b    on_3b  pitch_number  pitcher_pitch_number  prev_release_speed  prev_release_spin_rate  prev_plate_x  prev_plate_z  prev_hit_distance_sc  outs_when_up_0.0  outs_when_up_1.0  outs_when_up_2.0  count_0-0  count_0-1  count_0-2  count_1-0  count_1-1  count_1-2  count_2-0  count_2-1  count_2-2  count_3-0  count_3-1  count_3-2  stand_R  p_throws_R  prev_pitch_class_changeup  prev_pitch_class_curveball  prev_pitch_class_cutter  prev_pitch_class_fourseam  prev_pitch_class_nan  prev_pitch_class_slider  prev_pitch_class_twoseam  prev_outcome_ball  prev_outcome_called_strike  prev_outcome_foul  \\\n",
       "0               -0.78043               0.58270             -0.44229             -1.03107                 1.25860                0.97975                  -0.91678                  0.30666                -0.45978                -1.11133                    2.55008                   0.58950                   0.36672                    0.75330                -0.80604                   0.29085                -2.19225                  0.45772                       0.76729                        0.26517                    -0.80229                      -0.15407                    -2.12834                      0.11610                1.01428                 1.01829             -0.74962               -3.94565             -1.92348               0.57852              -0.00953                1.02244            -0.53746              -1.24991            -1.27481              0.35641                0.95449                -0.29283             -0.54859               -1.57796             -1.03809               0.03741                      4.60990                       0.39562                   -0.37305                     -1.42516                   -1.93447                    -1.20335                      -1.56294                       -0.21872                     0.25102                      -1.96208                     2.23426                     -2.02054   -0.84708   -0.85320 -0.65517 -0.46297 -0.31102      -1.10105              -1.17847           -10.37162                -4.97929      -0.05153      -2.34281              -0.45101           1.00000           0.00000           0.00000    1.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000  0.00000     1.00000                    0.00000                     0.00000                  0.00000                    0.00000               1.00000                  0.00000                   0.00000            0.00000                     0.00000            0.00000   \n",
       "1               -0.78043               0.58270             -0.44229             -1.03107                 1.25860                0.97975                  -0.91678                  0.30666                -0.45978                -1.11133                    2.55008                   0.58950                   0.36672                    0.75330                -0.80604                   0.29085                -2.19225                  0.46121                       0.76729                        0.26517                    -0.80229                      -0.15407                    -2.12834                      0.29116                1.01428                 1.01829             -0.74962               -3.94565             -1.92348               1.83526              -0.00953                1.02244            -0.53746              -1.24991            -1.27481             -0.81058                0.95449                -0.29283             -0.54859               -1.57796             -1.03809              -1.27761                      4.60990                       0.39562                   -0.37305                     -1.42516                   -1.93447                    -1.20335                      -1.56294                       -0.21872                     0.25102                      -1.96208                     2.23426                     -2.02054   -0.84708   -0.85320 -0.65517 -0.46297 -0.31102      -0.52608              -1.14384             0.78082                -0.08263       0.27353       2.19093              -0.45101           1.00000           0.00000           0.00000    0.00000    0.00000    0.00000    1.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000  0.00000     1.00000                    0.00000                     0.00000                  0.00000                    1.00000               0.00000                  0.00000                   0.00000            1.00000                     0.00000            0.00000   \n",
       "2               -0.92706               0.73618             -0.44229             -1.03107                 1.25860                0.97975                  -0.91678                  0.35530                -0.45978                -1.11133                    2.46201                   0.58950                   0.36672                    0.75330                -0.80604                   0.29085                -2.19225                  0.48617                       0.76729                        0.26517                    -0.80229                      -0.15407                    -2.12834                      0.19522                1.01428                 1.01829             -0.74962               -3.94565             -1.92348              -0.25931              -0.00953                1.02244            -0.53746              -1.24991            -1.27481             -0.81058                0.95449                -0.29283             -0.54859               -1.57796             -1.03809              -1.27761                      4.60990                       0.39562                   -0.37305                     -1.42516                   -1.93447                    -1.20335                      -1.56294                       -0.21872                     0.25102                      -1.96208                     2.23426                     -2.02054   -0.84708   -0.85320 -0.65517 -0.46297 -0.31102       0.04889              -1.10922             0.89834                -0.08719      -0.72488       0.08636              -0.45101           1.00000           0.00000           0.00000    0.00000    0.00000    0.00000    0.00000    1.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000  0.00000     1.00000                    0.00000                     0.00000                  0.00000                    0.00000               0.00000                  0.00000                   1.00000            0.00000                     0.00000            1.00000   \n",
       "3               -0.92706               0.88965             -0.44229             -1.03107                 0.98958                0.97975                  -0.91678                  0.40394                -0.45978                -1.11133                    2.37394                   0.58950                   0.36672                    0.75330                -0.80604                   0.29085                 0.45495                  0.48617                       0.76729                        0.26517                    -0.80229                      -0.15407                     0.26505                      0.19522                1.01428                 1.01829             -0.74962               -3.94565              1.96877              -0.25931              -0.00953                1.02244            -0.53746              -1.24991            -1.27481             -0.81058                0.95449                -0.29283             -0.54859               -1.57796              2.98801              -1.27761                      4.60990                       0.39562                   -0.37305                     -1.42516                   -1.93447                    -1.20335                      -1.56294                       -0.21872                     0.25102                      -1.96208                     3.30663                     -2.02054   -0.84708   -0.85320 -0.65517 -0.46297 -0.31102       0.62385              -1.07459             1.06287                -0.38154      -0.06314      -0.95023              -0.45101           1.00000           0.00000           0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    1.00000    0.00000    0.00000    0.00000    0.00000  0.00000     1.00000                    0.00000                     0.00000                  0.00000                    0.00000               0.00000                  0.00000                   1.00000            1.00000                     0.00000            0.00000   \n",
       "4               -1.07369               0.88965             -0.44229             -0.82157                 0.98958                0.97975                  -0.91678                  0.40394                -0.45978                -1.04349                    2.37394                   0.49306                   0.36672                    0.75330                -0.80604                   0.29085                 0.45495                  0.50756                       0.76729                        0.26517                    -0.80229                      -0.15407                     0.26505                      0.18853                1.01428                 1.01829             -0.74962               -3.94565              1.96877               0.43888              -0.00953                1.02244            -0.53746              -1.24991            -1.27481             -0.81058                0.95449                -0.29283             -0.54859               -1.57796              2.98801              -1.27761                      0.63669                       0.32120                   -0.49321                     -0.30970                    1.63676                    -1.20335                      -0.59329                        0.87771                     0.55227                       1.32536                    -0.06686                     -2.02054   -0.84708   -0.85320  1.52631 -0.46297 -0.31102      -1.10105              -1.03997            -0.42961                 0.02690       0.11100      -0.31152               2.23651           1.00000           0.00000           0.00000    1.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000    0.00000  1.00000     1.00000                    0.00000                     0.00000                  0.00000                    0.00000               0.00000                  1.00000                   0.00000            0.00000                     0.00000            0.00000   \n",
       "\n",
       "   prev_outcome_other  prev_outcome_out  prev_outcome_swinging_strike  tto_1.0  tto_2.0  tto_3.0  tto_4.0  tto_5.0      game_pk  at_bat_number  \n",
       "0             1.00000           0.00000                       0.00000  1.00000  0.00000  0.00000  0.00000  0.00000 490099.00000        1.00000  \n",
       "1             0.00000           0.00000                       0.00000  1.00000  0.00000  0.00000  0.00000  0.00000 490099.00000        1.00000  \n",
       "2             0.00000           0.00000                       0.00000  1.00000  0.00000  0.00000  0.00000  0.00000 490099.00000        1.00000  \n",
       "3             0.00000           0.00000                       0.00000  1.00000  0.00000  0.00000  0.00000  0.00000 490099.00000        1.00000  \n",
       "4             1.00000           0.00000                       0.00000  1.00000  0.00000  0.00000  0.00000  0.00000 490099.00000        2.00000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785777, 103)\n"
     ]
    }
   ],
   "source": [
    "# preview the transformed training data\n",
    "display(x_train_transformed.head())\n",
    "\n",
    "# print the shape of the training data\n",
    "print(x_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDzjTAeIP1HV"
   },
   "source": [
    "### Input and Output Sequence Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0kPYc08P9mZ"
   },
   "source": [
    "To train the LSTM w/ attention model, we need to pass in the input and output data as sequences. Each sequence to the model will represent an at-bat. All input and output sequences in the training data need to be of the same length during training. We will pad the input and output data with zeros so that each sequence is 21 pitches long. We will only perform the transformation to the training and validation data sets as these will be the datasets passed through the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 80199,
     "status": "ok",
     "timestamp": 1639250413864,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "a87O_rTA36S0"
   },
   "outputs": [],
   "source": [
    "##### Prepare List of Input Sequences -----\n",
    "\n",
    "# train\n",
    "# split data by game id and atbat\n",
    "x_train_splits = list(x_train_transformed.groupby([\"game_pk\", \"at_bat_number\"]))\n",
    "x_train_sequences = [torch.tensor(np.array(x_train_splits[item][1])[:,:x_train_transformed.shape[1]-2], dtype=torch.float) for item in range(len(x_train_splits))] # dont want the game id and at bat number so dropping the last two columns\n",
    "\n",
    "# validation\n",
    "# split data by game id and atbat\n",
    "x_validation_splits = list(x_validation_transformed.groupby([\"game_pk\", \"at_bat_number\"]))\n",
    "x_validation_sequences = [torch.tensor(np.array(x_validation_splits[item][1])[:,:x_train_transformed.shape[1]-2], dtype=torch.float) for item in range(len(x_validation_splits))]\n",
    "\n",
    "# test\n",
    "# split data by game id and atbat\n",
    "x_test_splits = list(x_test_transformed.groupby([\"game_pk\", \"at_bat_number\"]))\n",
    "x_test_sequences = [torch.tensor(np.array(x_test_splits[item][1])[:,:x_train_transformed.shape[1]-2], dtype=torch.float) for item in range(len(x_test_splits))]\n",
    "\n",
    "\n",
    "##### Prepare List of Output Sequences -----\n",
    "\n",
    "# Assign each pitch with a unique index\n",
    "pitch_to_ix = {\"fourseam\": 1, \"twoseam\": 2, \"slider\": 3,\n",
    "               \"changeup\": 4, \"curveball\": 5, \"cutter\": 6}\n",
    "\n",
    "# create a function to convert the pitch code to the index\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# train\n",
    "# split data by game id and atbat\n",
    "y_train_splits = list(y_train.groupby([\"game_pk\", \"at_bat_number\"]))\n",
    "y_train_sequences = [prepare_sequence(np.array(y_train_splits[item][1])[:,2], pitch_to_ix) for item in range(len(y_train_splits))]\n",
    "\n",
    "# validation\n",
    "# split data by game id and atbat\n",
    "y_validation_splits = list(y_validation.groupby([\"game_pk\", \"at_bat_number\"]))\n",
    "y_validation_sequences = [prepare_sequence(np.array(y_validation_splits[item][1])[:,2], pitch_to_ix) for item in range(len(y_validation_splits))]\n",
    "\n",
    "# test\n",
    "# split data by game id and atbat\n",
    "y_test_splits = list(y_test.groupby([\"game_pk\", \"at_bat_number\"]))\n",
    "y_test_sequences = [prepare_sequence(np.array(y_test_splits[item][1])[:,2], pitch_to_ix) for item in range(len(y_test_splits))]\n",
    "\n",
    "\n",
    "##### Pad Sequences -----\n",
    "\n",
    "# train\n",
    "x_train_padded_sequences = pad_sequence(x_train_sequences, batch_first=True)\n",
    "y_train_padded_sequences = pad_sequence(y_train_sequences, batch_first=True)\n",
    "\n",
    "# validation\n",
    "x_validation_padded_sequences = pad_sequence(x_validation_sequences, batch_first=True)\n",
    "y_validation_padded_sequences = pad_sequence(y_validation_sequences, batch_first=True)\n",
    "\n",
    "# test\n",
    "x_test_padded_sequences = pad_sequence(x_test_sequences, batch_first=True)\n",
    "y_test_padded_sequences = pad_sequence(y_test_sequences, batch_first=True)\n",
    "\n",
    "# get the lengths of the input sequences\n",
    "x_train_lengths = [item.size()[0] for item in x_train_sequences]\n",
    "x_validation_lengths = [item.size()[0] for item in x_validation_sequences]\n",
    "x_test_lengths = [item.size()[0] for item in x_test_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPdoz7IpNlfI"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1639250413866,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "lOiqc--ifqtc"
   },
   "outputs": [],
   "source": [
    "# create a custom class to return tensors that will be inputs into deep learning model\n",
    "class CustomLSTMDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data_1, X_data_2, y_data):\n",
    "        self.X_data_1 = torch.FloatTensor(X_data_1)\n",
    "        self.X_data_2 = torch.FloatTensor(X_data_2)\n",
    "        self.y_data = torch.LongTensor(y_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data_1[index], self.X_data_2[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1639250413866,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "36ucbQSafw6i"
   },
   "outputs": [],
   "source": [
    "#### Data Loaders ----\n",
    "\n",
    "# set batch size\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "# return tensors\n",
    "train_data = CustomLSTMDataset(x_train_padded_sequences, x_train_lengths, y_train_padded_sequences)\n",
    "\n",
    "# return tensors\n",
    "validation_data = CustomLSTMDataset(x_validation_padded_sequences, x_validation_lengths, y_validation_padded_sequences)\n",
    "\n",
    "# return tensors\n",
    "test_data = CustomLSTMDataset(x_test_padded_sequences, x_test_lengths, y_test_padded_sequences)\n",
    "\n",
    "# set training data loader\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# set validation data loader\n",
    "validation_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# set test data loader\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1639250413867,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "bFR2CeRNbqKi"
   },
   "outputs": [],
   "source": [
    "#### Define training functions ----\n",
    "\n",
    "# define function to train a model at each epoch and return train loss and accuracy\n",
    "def train_epoch(epoch, clf, criterion, opt, data_loader, printlog=False, printnum=1):\n",
    "\n",
    "  \"\"\"Train pytorch model and collect training results\n",
    "  \n",
    "  Keyword arguments:\n",
    "  \n",
    "  epoch -- number of epochs\n",
    "  clf -- torch nn module\n",
    "  criterion -- loss function\n",
    "  opt -- torch optimizer function\n",
    "  data_loader -- torch train data loader\n",
    "  printlog -- indicator to print the training loss and accuracy (default=False)\n",
    "  printnum -- print the output after this number of epochs\n",
    "  \"\"\"\n",
    "\n",
    "  # training mode\n",
    "  clf.train()\n",
    "\n",
    "  # reset train_loss, correct predictions, and predictions\n",
    "  correct = 0.0\n",
    "  n = 0.0\n",
    "  train_loss = 0.0\n",
    "\n",
    "  # training via batching\n",
    "  for batch_id, data in enumerate(data_loader):\n",
    "      \n",
    "      # load in input and target data in batch\n",
    "      input, input_length, target = data[0].permute(1,0,2).to(device), data[1].to(device), data[2].permute(1,0).to(device)\n",
    "      \n",
    "      # predict the train data\n",
    "      y_pred = clf(input, input_length, target)\n",
    "      \n",
    "      # reshape the prediction and target so they can be compared for calculating loss\n",
    "      y_pred = y_pred.reshape(-1, y_pred.shape[2])\n",
    "      target = target.reshape(-1)\n",
    "\n",
    "      # filter out the padded inputs from the loss calculation\n",
    "      y_pred = y_pred[target > 0]\n",
    "      target = target[target > 0] - 1 # reindexing values to calculate loss\n",
    "\n",
    "      # calculate loss\n",
    "      loss = criterion(y_pred, target)\n",
    "\n",
    "      # zero the gradient\n",
    "      opt.zero_grad()\n",
    "\n",
    "      # backpropogation\n",
    "      loss.backward()\n",
    "\n",
    "      # gradient clipping\n",
    "      torch.nn.utils.clip_grad_norm_(clf.parameters(), max_norm=1)\n",
    "\n",
    "      # update model weights\n",
    "      opt.step()\n",
    "\n",
    "      # append loss stat\n",
    "      train_loss += loss.item()\n",
    "\n",
    "      # returns class prediction\n",
    "      pred_class = y_pred.argmax(1)\n",
    "\n",
    "      # number of correct predictions\n",
    "      correct += (pred_class == target).sum().item()\n",
    "\n",
    "      # append number of predictions made\n",
    "      n += target.size(0)\n",
    "\n",
    "  # print run stats at end of each epoch for average loss, accuracy\n",
    "  if (printlog == True) & (epoch % printnum == 0):\n",
    "    print(f'Epoch {epoch}: | Train Loss: {train_loss/len(data_loader):.5f} | Train Acc: {correct/n:.5f}')\n",
    "\n",
    "  train_loss = train_loss/len(data_loader) # running loss\n",
    "  train_accuracy = correct/n # train accuracy for epoch\n",
    "\n",
    "  return clf, train_loss, train_accuracy\n",
    "\n",
    "# define function to evaluate a model at each epoch and return test loss and accuracy\n",
    "def evaluate_model(epoch, clf, criterion, opt, data_loader, printlog=False, printnum=1):\n",
    "\n",
    "  \"\"\"Evaluate pytorch model and collect test results\n",
    "  \n",
    "  Keyword arguments:\n",
    "  \n",
    "  epoch -- number of epochs\n",
    "  clf -- torch nn module\n",
    "  criterion -- loss function\n",
    "  opt -- torch optimizer function\n",
    "  data_loader -- torch test data loader\n",
    "  printlog -- indicator to print the test loss and accuracy (default=False)\n",
    "  printnum -- print the output after this number of epochs\n",
    "  \"\"\"\n",
    "\n",
    "  clf.eval() # set model in inference mode\n",
    "  \n",
    "  # set test_loss, correct predictions, and predictions\n",
    "  test_loss = 0.0\n",
    "  correct = 0.0\n",
    "  n = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i,data in enumerate(data_loader):\n",
    "\n",
    "        # load in input and target data in batch\n",
    "        input, input_length, target = data[0].permute(1,0,2).to(device), data[1].to(device), data[2].permute(1,0).to(device)\n",
    "        \n",
    "        # predict the train data\n",
    "        y_pred = clf(input, input_length, target)\n",
    "        \n",
    "        # reshape prediction and target for calculating loss\n",
    "        y_pred = y_pred.reshape(-1, y_pred.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        # remove the padded values from the loss calculation\n",
    "        y_pred = y_pred[target > 0]\n",
    "        target = target[target > 0] - 1 # reindexing values to calculate loss\n",
    "\n",
    "        # calculate loss by removing the padded inputs\n",
    "        loss = criterion(y_pred, target)\n",
    "\n",
    "        # append loss stat\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # returns class prediction\n",
    "        pred_class = y_pred.argmax(1)\n",
    "\n",
    "        # number of correct predictions\n",
    "        correct += (pred_class == target).sum().item()\n",
    "          \n",
    "        # append number of predictions made\n",
    "        n += target.size(0)\n",
    "\n",
    "  # print run stats at end of each epoch for average loss, accuracy\n",
    "  if (printlog == True) & (epoch % printnum) == 0:\n",
    "    print(f'Epoch {epoch}: | Test Loss: {test_loss/len(data_loader):.5f} | Test Acc: {correct/n:.5f}')\n",
    "\n",
    "  test_loss = test_loss/len(data_loader) # running loss\n",
    "  test_accuracy = correct/n # test accuracy at each epoch\n",
    "\n",
    "  return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "##### Define training and evaluation function wrapper ----\n",
    "\n",
    "# define a function to perform training given classifier, hidden layers, activation function, \n",
    "# initialization technique, and optimizer arguments\n",
    "def train_and_evaluate(clf, opt, epochs, learning_rate, criterion, printlog, train_data_loader, test_data_loader, printnum):\n",
    "\n",
    "  \"\"\"Train and Evaluate pytorch deep neural network model given classifier, \n",
    "  number of hidden layers, activation function, weight initialization strategy, \n",
    "  and training parameters.\n",
    "  \n",
    "  Keyword arguments:\n",
    "  \n",
    "  clf -- torch nn module\n",
    "  opt -- torch optimizer function\n",
    "  epochs -- number of epochs\n",
    "  learning_rate -- learning rate for weight optimization\n",
    "  criterion -- loss function\n",
    "  printlog -- indicator to print the training and test loss and accuracy (default=False)\n",
    "  train_data_loader -- the training data loader\n",
    "  test_data_loader -- the out of sample data loader\n",
    "  printnum -- number of epochs run to print the loss\n",
    "  \"\"\"\n",
    "\n",
    "  # place classifier on the device\n",
    "  clf.to(device)\n",
    "\n",
    "  # show classifier architecture\n",
    "  print(clf)\n",
    "\n",
    "  # set optimizer\n",
    "  opt = opt(clf.parameters(), lr=learning_rate)\n",
    "  \n",
    "  # set loss function\n",
    "  criterion = criterion()\n",
    "\n",
    "  # initialize vectors to store performance\n",
    "  train_loss = []\n",
    "  train_accuracy = []\n",
    "  test_loss = []\n",
    "  test_accuracy = []\n",
    "\n",
    "  # initialize the early_stopping object\n",
    "  early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "  # train the model using parameters\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "      clf, train_loss_epoch, train_accuracy_epoch = train_epoch(epoch, clf, criterion, opt, train_data_loader, printlog, printnum)\n",
    "      test_loss_epoch, test_accuracy_epoch = evaluate_model(epoch, clf, criterion, opt, test_data_loader, printlog, printnum)\n",
    "      train_loss.append(train_loss_epoch)\n",
    "      train_accuracy.append(train_accuracy_epoch)\n",
    "      test_loss.append(test_loss_epoch)\n",
    "      test_accuracy.append(test_accuracy_epoch)\n",
    "      \n",
    "      early_stopping(test_loss_epoch, clf)\n",
    "        \n",
    "      if early_stopping.early_stop:\n",
    "          print(\"Early stopping\")\n",
    "          break\n",
    "\n",
    "      # load the last checkpoint with the best model\n",
    "      clf.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "  return clf, train_loss, train_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBKLlI0SN8Yu"
   },
   "source": [
    "The network model attempted is an LSTM w/ attention mechanism network which leverages the current game state, previous pitch information, and historical pitcher/batter statistics. The hypothesis is that incorporating previous pitch information in the model will improve over the baseline model which does not learn over the sequence of pitches within the at-bat.\n",
    "\n",
    "The pitch sequence input is passed to an LSTM encoder layer with hidden size 300. The hidden states are then weighted based on the attention scores to generate the context vector. The context vector is then concatenated with the previous state input before being passed into the decoder. The decoder network then generates the next pitch prediction using an LSTM and dense layer. The LSTM layers in the encoder and decoder have a dropout rate of 0.25. A series of hyperparameters were adjusted to determine which set of hyperparameters lead to the best validation accuracy. The experiment log is listed in the training section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1639250413868,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "ikAXKL-2Xfev"
   },
   "outputs": [],
   "source": [
    "##### Define seq2seq model w/ attention ----\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # set model params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # specify the lstm encoder\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, x_lengths, t):\n",
    "\n",
    "        # set the sequence length for padding the input to set size\n",
    "        seq_length = t+1\n",
    "\n",
    "        x_lengths = np.array(x_lengths.cpu().detach().numpy())\n",
    "\n",
    "        x_lengths = np.where(x_lengths > seq_length, seq_length, x_lengths)\n",
    "\n",
    "        x_lengths = torch.tensor(x_lengths)\n",
    "\n",
    "        # pack the padded input sequence\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x[:t+1,:,:], x_lengths, enforce_sorted=False)\n",
    "        \n",
    "        # run through lstm\n",
    "        encoder_states, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # pad the input sequence\n",
    "        encoder_states, _ = torch.nn.utils.rnn.pad_packed_sequence(encoder_states, total_length=seq_length)\n",
    "        \n",
    "        return encoder_states, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # set model params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # set decoder architecture\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        # set attention architecture\n",
    "        self.energy = nn.Linear(hidden_size * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # regularization\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "        # activations\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        \n",
    "        # get target data in shape [1, batch size]\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # embed the previous target in shape [1, batch size, embedding size]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # get the sequence length\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "\n",
    "        # reshape the hidden state to shape [sequence length, batch size, hidden size]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "\n",
    "        # calculate the energy for the attention inputs of shape [sequence length, batch size, 1]\n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "\n",
    "        # calculate attention of shape [sequence length, batch size, 1]\n",
    "        attention = self.softmax(energy)\n",
    "\n",
    "        # generate the context vector of shape [1, batch size, hidden size*2]\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "\n",
    "        # concatenate the context vector and the embedding of previous output [1, batch size, hidden size + embedding size]\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "\n",
    "        # generate the output of shape [1, batch size, hidden size]\n",
    "        outputs, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "\n",
    "        # reshape the output to get the predictions of shape [batch size, hidden size]\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, n_classes):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        # set encoder, decoder, and target classes\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.target_output_size = n_classes\n",
    "\n",
    "    def forward(self, source, x_lens, target, teacher_force_ratio=0.5):\n",
    "        \n",
    "        # set batch size and target shape\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "\n",
    "        # initialise a tensor for storing the model predictions at each time step\n",
    "        outputs = torch.zeros(target_len, batch_size, self.target_output_size).to(device)\n",
    "\n",
    "        # first pitch of at-bat has no prior target so we set to pad index of 0\n",
    "        x = torch.zeros_like(target[0])\n",
    "\n",
    "        # for each pitch leverage the attention of previous pitches to predict target\n",
    "        for t in range(target_len):\n",
    "            \n",
    "            # run input through the encoder\n",
    "            encoder_states, hidden, cell = self.encoder(source, x_lens, t)\n",
    "\n",
    "            # at every time step use decoder to generate the new output with encoder states and updated hidden, cell states\n",
    "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
    "\n",
    "            # store prediction for current time step\n",
    "            outputs[t] = output\n",
    "\n",
    "            # get index of decoder prediction\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # Use teacher forcing for the model to be accustomed to seeing\n",
    "            # similar inputs at training and evaluation steps\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsXgsFxkyYui"
   },
   "source": [
    "The GPU instance needs to be setup to allow for faster training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1639250413868,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "BwEyBy7pyYuj",
    "outputId": "df553343-723c-4d16-abba-e547a6fbb971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training will occur using a cuda device\n"
     ]
    }
   ],
   "source": [
    "#### Set up the GPU instance ----\n",
    "\n",
    "# set up GPU instance\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# we should be printing out CUDA\n",
    "print(f\"Training will occur using a {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBLviZWJNrYA"
   },
   "source": [
    "Let's train the model using the adam optimizer with learning rate of 0.0001 over 200 epochs. The loss and accuracy will be printed out at each epoch. Early stopping is applied which stops the model from training further if there has not been a decrease in the validation loss after 20 consecutive epochs. The best performing model is specified below after executing experiments adjusting different hyperparameters. The LSTM model with the attention mechanism was tuned with varying batch sizes (2,000 or 3,000), number of layers (1 to 2), hidden layer size (300 or 500), dropout (.2 to .4),  and learning rate (.001 to .0001). Early stopping and Softmax activation was applied to the final layer similar to the fully connected and LSTM networks. The best performing architecture leveraged the current game state, historical batter/pitcher stats, and previous pitch type. The final architecture used batch size 2,000, 1 hidden layer, hidden size 300, encoder and decoder dropout rate .25, and learning rate .0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2876289,
     "status": "ok",
     "timestamp": 1639253290146,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "X7izscqnfVjm",
    "outputId": "5858f62a-aa5a-43ce-e527-c3a159f0051e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(101, 300)\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(7, 10)\n",
      "    (lstm): LSTM(310, 300)\n",
      "    (energy): Linear(in_features=600, out_features=1, bias=True)\n",
      "    (fc): Linear(in_features=300, out_features=6, bias=True)\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "    (softmax): Softmax(dim=0)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "Epoch 0: | Train Loss: 1.59080 | Train Acc: 0.41335\n",
      "Epoch 0: | Test Loss: 1.40371 | Test Acc: 0.44173\n",
      "Validation loss decreased (inf --> 1.403708).  Saving model ...\n",
      "Epoch 1: | Train Loss: 1.31415 | Train Acc: 0.46421\n",
      "Epoch 1: | Test Loss: 1.28596 | Test Acc: 0.45964\n",
      "Validation loss decreased (1.403708 --> 1.285958).  Saving model ...\n",
      "Epoch 2: | Train Loss: 1.25391 | Train Acc: 0.47089\n",
      "Epoch 2: | Test Loss: 1.25350 | Test Acc: 0.46384\n",
      "Validation loss decreased (1.285958 --> 1.253502).  Saving model ...\n",
      "Epoch 3: | Train Loss: 1.22880 | Train Acc: 0.47475\n",
      "Epoch 3: | Test Loss: 1.23503 | Test Acc: 0.46680\n",
      "Validation loss decreased (1.253502 --> 1.235030).  Saving model ...\n",
      "Epoch 4: | Train Loss: 1.21336 | Train Acc: 0.47710\n",
      "Epoch 4: | Test Loss: 1.22311 | Test Acc: 0.46828\n",
      "Validation loss decreased (1.235030 --> 1.223110).  Saving model ...\n",
      "Epoch 5: | Train Loss: 1.20311 | Train Acc: 0.47933\n",
      "Epoch 5: | Test Loss: 1.21489 | Test Acc: 0.46949\n",
      "Validation loss decreased (1.223110 --> 1.214885).  Saving model ...\n",
      "Epoch 6: | Train Loss: 1.19562 | Train Acc: 0.48071\n",
      "Epoch 6: | Test Loss: 1.20878 | Test Acc: 0.47048\n",
      "Validation loss decreased (1.214885 --> 1.208777).  Saving model ...\n",
      "Epoch 7: | Train Loss: 1.18958 | Train Acc: 0.48212\n",
      "Epoch 7: | Test Loss: 1.20432 | Test Acc: 0.47075\n",
      "Validation loss decreased (1.208777 --> 1.204320).  Saving model ...\n",
      "Epoch 8: | Train Loss: 1.18469 | Train Acc: 0.48350\n",
      "Epoch 8: | Test Loss: 1.20040 | Test Acc: 0.47212\n",
      "Validation loss decreased (1.204320 --> 1.200396).  Saving model ...\n",
      "Epoch 9: | Train Loss: 1.18060 | Train Acc: 0.48450\n",
      "Epoch 9: | Test Loss: 1.19684 | Test Acc: 0.47348\n",
      "Validation loss decreased (1.200396 --> 1.196838).  Saving model ...\n",
      "Epoch 10: | Train Loss: 1.17677 | Train Acc: 0.48576\n",
      "Epoch 10: | Test Loss: 1.19423 | Test Acc: 0.47328\n",
      "Validation loss decreased (1.196838 --> 1.194234).  Saving model ...\n",
      "Epoch 11: | Train Loss: 1.17377 | Train Acc: 0.48645\n",
      "Epoch 11: | Test Loss: 1.19163 | Test Acc: 0.47410\n",
      "Validation loss decreased (1.194234 --> 1.191632).  Saving model ...\n",
      "Epoch 12: | Train Loss: 1.17087 | Train Acc: 0.48733\n",
      "Epoch 12: | Test Loss: 1.18965 | Test Acc: 0.47479\n",
      "Validation loss decreased (1.191632 --> 1.189652).  Saving model ...\n",
      "Epoch 13: | Train Loss: 1.16817 | Train Acc: 0.48849\n",
      "Epoch 13: | Test Loss: 1.18797 | Test Acc: 0.47468\n",
      "Validation loss decreased (1.189652 --> 1.187969).  Saving model ...\n",
      "Epoch 14: | Train Loss: 1.16563 | Train Acc: 0.48947\n",
      "Epoch 14: | Test Loss: 1.18687 | Test Acc: 0.47529\n",
      "Validation loss decreased (1.187969 --> 1.186866).  Saving model ...\n",
      "Epoch 15: | Train Loss: 1.16337 | Train Acc: 0.49047\n",
      "Epoch 15: | Test Loss: 1.18449 | Test Acc: 0.47580\n",
      "Validation loss decreased (1.186866 --> 1.184489).  Saving model ...\n",
      "Epoch 16: | Train Loss: 1.16112 | Train Acc: 0.49102\n",
      "Epoch 16: | Test Loss: 1.18325 | Test Acc: 0.47603\n",
      "Validation loss decreased (1.184489 --> 1.183255).  Saving model ...\n",
      "Epoch 17: | Train Loss: 1.15902 | Train Acc: 0.49206\n",
      "Epoch 17: | Test Loss: 1.18192 | Test Acc: 0.47644\n",
      "Validation loss decreased (1.183255 --> 1.181918).  Saving model ...\n",
      "Epoch 18: | Train Loss: 1.15701 | Train Acc: 0.49276\n",
      "Epoch 18: | Test Loss: 1.18091 | Test Acc: 0.47666\n",
      "Validation loss decreased (1.181918 --> 1.180913).  Saving model ...\n",
      "Epoch 19: | Train Loss: 1.15509 | Train Acc: 0.49358\n",
      "Epoch 19: | Test Loss: 1.18022 | Test Acc: 0.47726\n",
      "Validation loss decreased (1.180913 --> 1.180218).  Saving model ...\n",
      "Epoch 20: | Train Loss: 1.15319 | Train Acc: 0.49416\n",
      "Epoch 20: | Test Loss: 1.17866 | Test Acc: 0.47780\n",
      "Validation loss decreased (1.180218 --> 1.178661).  Saving model ...\n",
      "Epoch 21: | Train Loss: 1.15143 | Train Acc: 0.49500\n",
      "Epoch 21: | Test Loss: 1.17871 | Test Acc: 0.47720\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 22: | Train Loss: 1.15145 | Train Acc: 0.49517\n",
      "Epoch 22: | Test Loss: 1.17814 | Test Acc: 0.47795\n",
      "Validation loss decreased (1.178661 --> 1.178137).  Saving model ...\n",
      "Epoch 23: | Train Loss: 1.14956 | Train Acc: 0.49589\n",
      "Epoch 23: | Test Loss: 1.17710 | Test Acc: 0.47777\n",
      "Validation loss decreased (1.178137 --> 1.177101).  Saving model ...\n",
      "Epoch 24: | Train Loss: 1.14771 | Train Acc: 0.49655\n",
      "Epoch 24: | Test Loss: 1.17649 | Test Acc: 0.47890\n",
      "Validation loss decreased (1.177101 --> 1.176488).  Saving model ...\n",
      "Epoch 25: | Train Loss: 1.14593 | Train Acc: 0.49754\n",
      "Epoch 25: | Test Loss: 1.17540 | Test Acc: 0.47883\n",
      "Validation loss decreased (1.176488 --> 1.175405).  Saving model ...\n",
      "Epoch 26: | Train Loss: 1.14428 | Train Acc: 0.49797\n",
      "Epoch 26: | Test Loss: 1.17443 | Test Acc: 0.47913\n",
      "Validation loss decreased (1.175405 --> 1.174432).  Saving model ...\n",
      "Epoch 27: | Train Loss: 1.14257 | Train Acc: 0.49871\n",
      "Epoch 27: | Test Loss: 1.17419 | Test Acc: 0.47965\n",
      "Validation loss decreased (1.174432 --> 1.174190).  Saving model ...\n",
      "Epoch 28: | Train Loss: 1.14060 | Train Acc: 0.49967\n",
      "Epoch 28: | Test Loss: 1.17491 | Test Acc: 0.47903\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 29: | Train Loss: 1.14049 | Train Acc: 0.49964\n",
      "Epoch 29: | Test Loss: 1.17287 | Test Acc: 0.47996\n",
      "Validation loss decreased (1.174190 --> 1.172868).  Saving model ...\n",
      "Epoch 30: | Train Loss: 1.13871 | Train Acc: 0.50008\n",
      "Epoch 30: | Test Loss: 1.17269 | Test Acc: 0.48003\n",
      "Validation loss decreased (1.172868 --> 1.172693).  Saving model ...\n",
      "Epoch 31: | Train Loss: 1.13700 | Train Acc: 0.50096\n",
      "Epoch 31: | Test Loss: 1.17193 | Test Acc: 0.47981\n",
      "Validation loss decreased (1.172693 --> 1.171933).  Saving model ...\n",
      "Epoch 32: | Train Loss: 1.13532 | Train Acc: 0.50168\n",
      "Epoch 32: | Test Loss: 1.17113 | Test Acc: 0.48073\n",
      "Validation loss decreased (1.171933 --> 1.171135).  Saving model ...\n",
      "Epoch 33: | Train Loss: 1.13347 | Train Acc: 0.50259\n",
      "Epoch 33: | Test Loss: 1.17082 | Test Acc: 0.48083\n",
      "Validation loss decreased (1.171135 --> 1.170823).  Saving model ...\n",
      "Epoch 34: | Train Loss: 1.13178 | Train Acc: 0.50326\n",
      "Epoch 34: | Test Loss: 1.17010 | Test Acc: 0.48118\n",
      "Validation loss decreased (1.170823 --> 1.170096).  Saving model ...\n",
      "Epoch 35: | Train Loss: 1.13016 | Train Acc: 0.50410\n",
      "Epoch 35: | Test Loss: 1.16957 | Test Acc: 0.48091\n",
      "Validation loss decreased (1.170096 --> 1.169567).  Saving model ...\n",
      "Epoch 36: | Train Loss: 1.12867 | Train Acc: 0.50470\n",
      "Epoch 36: | Test Loss: 1.16870 | Test Acc: 0.48106\n",
      "Validation loss decreased (1.169567 --> 1.168703).  Saving model ...\n",
      "Epoch 37: | Train Loss: 1.12704 | Train Acc: 0.50538\n",
      "Epoch 37: | Test Loss: 1.16834 | Test Acc: 0.48149\n",
      "Validation loss decreased (1.168703 --> 1.168340).  Saving model ...\n",
      "Epoch 38: | Train Loss: 1.12538 | Train Acc: 0.50572\n",
      "Epoch 38: | Test Loss: 1.16799 | Test Acc: 0.48160\n",
      "Validation loss decreased (1.168340 --> 1.167987).  Saving model ...\n",
      "Epoch 39: | Train Loss: 1.12385 | Train Acc: 0.50634\n",
      "Epoch 39: | Test Loss: 1.16688 | Test Acc: 0.48252\n",
      "Validation loss decreased (1.167987 --> 1.166884).  Saving model ...\n",
      "Epoch 40: | Train Loss: 1.12258 | Train Acc: 0.50716\n",
      "Epoch 40: | Test Loss: 1.16724 | Test Acc: 0.48150\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 41: | Train Loss: 1.12260 | Train Acc: 0.50745\n",
      "Epoch 41: | Test Loss: 1.16801 | Test Acc: 0.48150\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 42: | Train Loss: 1.12244 | Train Acc: 0.50705\n",
      "Epoch 42: | Test Loss: 1.16819 | Test Acc: 0.48076\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 43: | Train Loss: 1.12238 | Train Acc: 0.50729\n",
      "Epoch 43: | Test Loss: 1.16698 | Test Acc: 0.48205\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 44: | Train Loss: 1.12237 | Train Acc: 0.50743\n",
      "Epoch 44: | Test Loss: 1.16742 | Test Acc: 0.48201\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 45: | Train Loss: 1.12235 | Train Acc: 0.50719\n",
      "Epoch 45: | Test Loss: 1.16767 | Test Acc: 0.48197\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 46: | Train Loss: 1.12226 | Train Acc: 0.50777\n",
      "Epoch 46: | Test Loss: 1.16719 | Test Acc: 0.48196\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 47: | Train Loss: 1.12242 | Train Acc: 0.50715\n",
      "Epoch 47: | Test Loss: 1.16747 | Test Acc: 0.48142\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 48: | Train Loss: 1.12233 | Train Acc: 0.50748\n",
      "Epoch 48: | Test Loss: 1.16720 | Test Acc: 0.48158\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 49: | Train Loss: 1.12235 | Train Acc: 0.50752\n",
      "Epoch 49: | Test Loss: 1.16760 | Test Acc: 0.48157\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 50: | Train Loss: 1.12237 | Train Acc: 0.50733\n",
      "Epoch 50: | Test Loss: 1.16734 | Test Acc: 0.48195\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 51: | Train Loss: 1.12227 | Train Acc: 0.50740\n",
      "Epoch 51: | Test Loss: 1.16721 | Test Acc: 0.48204\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 52: | Train Loss: 1.12224 | Train Acc: 0.50727\n",
      "Epoch 52: | Test Loss: 1.16681 | Test Acc: 0.48221\n",
      "Validation loss decreased (1.166884 --> 1.166810).  Saving model ...\n",
      "Epoch 53: | Train Loss: 1.12091 | Train Acc: 0.50807\n",
      "Epoch 53: | Test Loss: 1.16650 | Test Acc: 0.48210\n",
      "Validation loss decreased (1.166810 --> 1.166501).  Saving model ...\n",
      "Epoch 54: | Train Loss: 1.11964 | Train Acc: 0.50859\n",
      "Epoch 54: | Test Loss: 1.16819 | Test Acc: 0.48171\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 55: | Train Loss: 1.11973 | Train Acc: 0.50867\n",
      "Epoch 55: | Test Loss: 1.16674 | Test Acc: 0.48131\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 56: | Train Loss: 1.11961 | Train Acc: 0.50874\n",
      "Epoch 56: | Test Loss: 1.16775 | Test Acc: 0.48191\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 57: | Train Loss: 1.11962 | Train Acc: 0.50887\n",
      "Epoch 57: | Test Loss: 1.16673 | Test Acc: 0.48204\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 58: | Train Loss: 1.11975 | Train Acc: 0.50824\n",
      "Epoch 58: | Test Loss: 1.16773 | Test Acc: 0.48179\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 59: | Train Loss: 1.11976 | Train Acc: 0.50853\n",
      "Epoch 59: | Test Loss: 1.16679 | Test Acc: 0.48220\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 60: | Train Loss: 1.11961 | Train Acc: 0.50864\n",
      "Epoch 60: | Test Loss: 1.16641 | Test Acc: 0.48176\n",
      "Validation loss decreased (1.166501 --> 1.166405).  Saving model ...\n",
      "Epoch 61: | Train Loss: 1.11837 | Train Acc: 0.50891\n",
      "Epoch 61: | Test Loss: 1.16705 | Test Acc: 0.48180\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 62: | Train Loss: 1.11847 | Train Acc: 0.50939\n",
      "Epoch 62: | Test Loss: 1.16651 | Test Acc: 0.48217\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 63: | Train Loss: 1.11853 | Train Acc: 0.50891\n",
      "Epoch 63: | Test Loss: 1.16719 | Test Acc: 0.48157\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 64: | Train Loss: 1.11852 | Train Acc: 0.50903\n",
      "Epoch 64: | Test Loss: 1.16625 | Test Acc: 0.48192\n",
      "Validation loss decreased (1.166405 --> 1.166248).  Saving model ...\n",
      "Epoch 65: | Train Loss: 1.11716 | Train Acc: 0.50973\n",
      "Epoch 65: | Test Loss: 1.16639 | Test Acc: 0.48190\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 66: | Train Loss: 1.11720 | Train Acc: 0.50983\n",
      "Epoch 66: | Test Loss: 1.16648 | Test Acc: 0.48221\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 67: | Train Loss: 1.11711 | Train Acc: 0.50989\n",
      "Epoch 67: | Test Loss: 1.16754 | Test Acc: 0.48221\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 68: | Train Loss: 1.11725 | Train Acc: 0.50991\n",
      "Epoch 68: | Test Loss: 1.16650 | Test Acc: 0.48182\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 69: | Train Loss: 1.11698 | Train Acc: 0.50968\n",
      "Epoch 69: | Test Loss: 1.16629 | Test Acc: 0.48154\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 70: | Train Loss: 1.11707 | Train Acc: 0.50975\n",
      "Epoch 70: | Test Loss: 1.16689 | Test Acc: 0.48158\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 71: | Train Loss: 1.11721 | Train Acc: 0.50967\n",
      "Epoch 71: | Test Loss: 1.16643 | Test Acc: 0.48216\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 72: | Train Loss: 1.11718 | Train Acc: 0.50983\n",
      "Epoch 72: | Test Loss: 1.16661 | Test Acc: 0.48222\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 73: | Train Loss: 1.11704 | Train Acc: 0.50980\n",
      "Epoch 73: | Test Loss: 1.16642 | Test Acc: 0.48169\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 74: | Train Loss: 1.11706 | Train Acc: 0.50965\n",
      "Epoch 74: | Test Loss: 1.16731 | Test Acc: 0.48176\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 75: | Train Loss: 1.11701 | Train Acc: 0.50953\n",
      "Epoch 75: | Test Loss: 1.16710 | Test Acc: 0.48207\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 76: | Train Loss: 1.11718 | Train Acc: 0.50991\n",
      "Epoch 76: | Test Loss: 1.16743 | Test Acc: 0.48158\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 77: | Train Loss: 1.11724 | Train Acc: 0.50950\n",
      "Epoch 77: | Test Loss: 1.16661 | Test Acc: 0.48155\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 78: | Train Loss: 1.11729 | Train Acc: 0.50952\n",
      "Epoch 78: | Test Loss: 1.16694 | Test Acc: 0.48213\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 79: | Train Loss: 1.11711 | Train Acc: 0.50986\n",
      "Epoch 79: | Test Loss: 1.16590 | Test Acc: 0.48277\n",
      "Validation loss decreased (1.166248 --> 1.165901).  Saving model ...\n",
      "Epoch 80: | Train Loss: 1.11583 | Train Acc: 0.51050\n",
      "Epoch 80: | Test Loss: 1.16616 | Test Acc: 0.48216\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 81: | Train Loss: 1.11585 | Train Acc: 0.51060\n",
      "Epoch 81: | Test Loss: 1.16643 | Test Acc: 0.48215\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 82: | Train Loss: 1.11586 | Train Acc: 0.51013\n",
      "Epoch 82: | Test Loss: 1.16686 | Test Acc: 0.48210\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 83: | Train Loss: 1.11584 | Train Acc: 0.51041\n",
      "Epoch 83: | Test Loss: 1.16661 | Test Acc: 0.48202\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 84: | Train Loss: 1.11601 | Train Acc: 0.51038\n",
      "Epoch 84: | Test Loss: 1.16680 | Test Acc: 0.48221\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 85: | Train Loss: 1.11609 | Train Acc: 0.51026\n",
      "Epoch 85: | Test Loss: 1.16720 | Test Acc: 0.48196\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 86: | Train Loss: 1.11583 | Train Acc: 0.51050\n",
      "Epoch 86: | Test Loss: 1.16612 | Test Acc: 0.48205\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 87: | Train Loss: 1.11601 | Train Acc: 0.51014\n",
      "Epoch 87: | Test Loss: 1.16597 | Test Acc: 0.48188\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 88: | Train Loss: 1.11600 | Train Acc: 0.51014\n",
      "Epoch 88: | Test Loss: 1.16654 | Test Acc: 0.48220\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 89: | Train Loss: 1.11596 | Train Acc: 0.51017\n",
      "Epoch 89: | Test Loss: 1.16629 | Test Acc: 0.48252\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 90: | Train Loss: 1.11592 | Train Acc: 0.51043\n",
      "Epoch 90: | Test Loss: 1.16611 | Test Acc: 0.48229\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 91: | Train Loss: 1.11573 | Train Acc: 0.51050\n",
      "Epoch 91: | Test Loss: 1.16637 | Test Acc: 0.48197\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 92: | Train Loss: 1.11597 | Train Acc: 0.51030\n",
      "Epoch 92: | Test Loss: 1.16643 | Test Acc: 0.48143\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 93: | Train Loss: 1.11589 | Train Acc: 0.51036\n",
      "Epoch 93: | Test Loss: 1.16673 | Test Acc: 0.48127\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 94: | Train Loss: 1.11616 | Train Acc: 0.51020\n",
      "Epoch 94: | Test Loss: 1.16572 | Test Acc: 0.48281\n",
      "Validation loss decreased (1.165901 --> 1.165721).  Saving model ...\n",
      "Epoch 95: | Train Loss: 1.11493 | Train Acc: 0.51068\n",
      "Epoch 95: | Test Loss: 1.16627 | Test Acc: 0.48209\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 96: | Train Loss: 1.11477 | Train Acc: 0.51096\n",
      "Epoch 96: | Test Loss: 1.16642 | Test Acc: 0.48200\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 97: | Train Loss: 1.11478 | Train Acc: 0.51082\n",
      "Epoch 97: | Test Loss: 1.16636 | Test Acc: 0.48247\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 98: | Train Loss: 1.11484 | Train Acc: 0.51084\n",
      "Epoch 98: | Test Loss: 1.16688 | Test Acc: 0.48191\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 99: | Train Loss: 1.11510 | Train Acc: 0.51093\n",
      "Epoch 99: | Test Loss: 1.16651 | Test Acc: 0.48165\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 100: | Train Loss: 1.11488 | Train Acc: 0.51064\n",
      "Epoch 100: | Test Loss: 1.16707 | Test Acc: 0.48189\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 101: | Train Loss: 1.11468 | Train Acc: 0.51092\n",
      "Epoch 101: | Test Loss: 1.16657 | Test Acc: 0.48210\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 102: | Train Loss: 1.11469 | Train Acc: 0.51069\n",
      "Epoch 102: | Test Loss: 1.16632 | Test Acc: 0.48182\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 103: | Train Loss: 1.11478 | Train Acc: 0.51106\n",
      "Epoch 103: | Test Loss: 1.16669 | Test Acc: 0.48173\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 104: | Train Loss: 1.11505 | Train Acc: 0.51081\n",
      "Epoch 104: | Test Loss: 1.16749 | Test Acc: 0.48272\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 105: | Train Loss: 1.11481 | Train Acc: 0.51099\n",
      "Epoch 105: | Test Loss: 1.16669 | Test Acc: 0.48154\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 106: | Train Loss: 1.11480 | Train Acc: 0.51048\n",
      "Epoch 106: | Test Loss: 1.16593 | Test Acc: 0.48217\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 107: | Train Loss: 1.11486 | Train Acc: 0.51070\n",
      "Epoch 107: | Test Loss: 1.16582 | Test Acc: 0.48207\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 108: | Train Loss: 1.11476 | Train Acc: 0.51099\n",
      "Epoch 108: | Test Loss: 1.16621 | Test Acc: 0.48197\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 109: | Train Loss: 1.11468 | Train Acc: 0.51053\n",
      "Epoch 109: | Test Loss: 1.16687 | Test Acc: 0.48156\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 110: | Train Loss: 1.11485 | Train Acc: 0.51092\n",
      "Epoch 110: | Test Loss: 1.16649 | Test Acc: 0.48111\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 111: | Train Loss: 1.11483 | Train Acc: 0.51085\n",
      "Epoch 111: | Test Loss: 1.16719 | Test Acc: 0.48245\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 112: | Train Loss: 1.11466 | Train Acc: 0.51099\n",
      "Epoch 112: | Test Loss: 1.16622 | Test Acc: 0.48199\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 113: | Train Loss: 1.11479 | Train Acc: 0.51094\n",
      "Epoch 113: | Test Loss: 1.16702 | Test Acc: 0.48256\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 114: | Train Loss: 1.11462 | Train Acc: 0.51082\n",
      "Epoch 114: | Test Loss: 1.16613 | Test Acc: 0.48172\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#### Train ----\n",
    "\n",
    "# Set the model hyperparameters\n",
    "input_size_decoder = 7 # set at 7 because of the 0 padding\n",
    "output_size = 6 # the number of classes\n",
    "input_size_encoder = 101 # the input size\n",
    "decoder_embedding_size = 10 # decode the targets as 10 dimensional embeddings\n",
    "hidden_size = 300\n",
    "num_layers = 1\n",
    "enc_dropout = 0.25\n",
    "dec_dropout = 0.25\n",
    "\n",
    "# set the encoder\n",
    "encoder_net = Encoder(input_size_encoder, hidden_size, num_layers, enc_dropout).to(device)\n",
    "\n",
    "# set the decoder\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
    "\n",
    "# set the sequence 2 sequence model\n",
    "attn_clf = Seq2Seq(encoder_net, decoder_net, output_size)\n",
    "\n",
    "# train and evaluate model with hyperparameters\n",
    "attn_clf, attn_train_loss, attn_train_accuracy, attn_test_loss, attn_test_accuracy = train_and_evaluate(attn_clf, optim.Adam, 200, 0.0001, nn.CrossEntropyLoss, True, train_loader, validation_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1639253290147,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "jK6JQYhfQBTG"
   },
   "outputs": [],
   "source": [
    "##### HP Tuning Results ----\n",
    "\n",
    "# Batch Size: 2000\n",
    "# Number of layers: 1\n",
    "# Hidden nodes: 500\n",
    "# Learning rate: .0001\n",
    "# Validation Accuracy: .47808\n",
    "\n",
    "# Batch Size: 2000\n",
    "# Number of layers: 1\n",
    "# Hidden nodes: 300\n",
    "# Learning rate: .0001\n",
    "# Validation Accuracy: .47788\n",
    "\n",
    "# Batch Size: 3000\n",
    "# Number of layers: 1\n",
    "# Hidden nodes: 500\n",
    "# Learning rate: .001\n",
    "# Validation Accuracy: .47806\n",
    "\n",
    "# Batch Size: 3000\n",
    "# Number of layers: 1\n",
    "# Hidden nodes: 300\n",
    "# Learning rate: .001\n",
    "# Validation Accuracy: .47889\n",
    "\n",
    "# No embeddings ----\n",
    "\n",
    "# Batch Size: 2000\n",
    "# Number of layers: 1\n",
    "# Hidden nodes: 300\n",
    "# Learning rate: .0001\n",
    "# Validation Accuracy: .48229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1639253290415,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "nh-cEJPkMW23",
    "outputId": "35fe9e86-7a8c-4141-97d6-4bc8d2c07b1a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Plotting the model accuracy over epochs ----\n",
    "\n",
    "plt.plot(attn_test_accuracy, '-g', label='test')\n",
    "plt.plot(attn_train_accuracy, '-b', label='train')\n",
    "plt.legend();\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\");\n",
    "plt.ylim(0, 1.005);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mFdhzdpeE_1"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 3011,
     "status": "ok",
     "timestamp": 1639253293423,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "ejVdcPl-Cxx7"
   },
   "outputs": [],
   "source": [
    "# predictions and target\n",
    "y_pred_final = []\n",
    "target_final = []\n",
    "\n",
    "# generate new and append predictions\n",
    "with torch.no_grad():\n",
    "  for i,data in enumerate(validation_loader):\n",
    "      \n",
    "      # load in input and target data in batch\n",
    "      input, input_length, target = data[0].permute(1,0,2).to(device), data[1].to(device), data[2].permute(1,0).to(device)\n",
    "        \n",
    "      # predict the train data\n",
    "      y_pred = attn_clf(input, input_length, target)\n",
    "      \n",
    "      # reshape prediction and target for calculating loss\n",
    "      y_pred = y_pred.reshape(-1, y_pred.shape[2])\n",
    "      target = target.reshape(-1)\n",
    "      \n",
    "      y_pred = y_pred[target > 0]\n",
    "      target = target[target > 0] - 1 # reindex\n",
    "      \n",
    "      # returns class prediction\n",
    "      pred_class = torch.max(y_pred.data, 1)[1]\n",
    "      \n",
    "      y_pred_final.append(pred_class.cpu().detach().numpy())\n",
    "\n",
    "      target_final.append(target.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1639253293423,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "HfRwHN9qDpTz"
   },
   "outputs": [],
   "source": [
    "# combine predictions and target into one array for comparison\n",
    "y_pred_final = np.concatenate(y_pred_final, axis=0)\n",
    "target_final = np.concatenate(target_final, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1639253293760,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "FKf55ownE_gA",
    "outputId": "c241ca71-20de-406b-e35a-7550765096e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7faa35530e10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Confusion Matrix -----\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix(target_final, y_pred_final))\n",
    "\n",
    "sns.heatmap(confusion_matrix_df, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1639253294726,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 420
    },
    "id": "UIHcwi01E_gB",
    "outputId": "a0c665de-6ef8-4deb-e1c2-e72b15bd2810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5202    0.6963    0.5955    119576\n",
      "           1     0.4863    0.5270    0.5059     56777\n",
      "           2     0.4544    0.3973    0.4239     58853\n",
      "           3     0.4082    0.2721    0.3266     46134\n",
      "           4     0.3870    0.2157    0.2770     38528\n",
      "           5     0.4464    0.3500    0.3924     25073\n",
      "\n",
      "    accuracy                         0.4818    344941\n",
      "   macro avg     0.4504    0.4097    0.4202    344941\n",
      "weighted avg     0.4682    0.4818    0.4652    344941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Classification Report -----\n",
    "\n",
    "print(classification_report(target_final, y_pred_final, digits=4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "07_LSTM_attention.ipynb",
   "provenance": [
    {
     "file_id": "1ToKP7ExcCJo-pOh2w0hOibHBZs_fvgkQ",
     "timestamp": 1637546885958
    },
    {
     "file_id": "1G6UjsXrkONozO_B_6dAAubDpmDh4CnZh",
     "timestamp": 1635011951588
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
