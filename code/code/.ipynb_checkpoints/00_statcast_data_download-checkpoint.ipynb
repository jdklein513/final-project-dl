{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av9Qmkk-XMsm"
   },
   "source": [
    "# Importing Statcast MLB Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knaAKF9sXUfd"
   },
   "source": [
    "**Author:** Jacob Sauberman, Joel Klein, Ben Perkins\n",
    "\n",
    "**Source:** pybaseball github repo\n",
    "\n",
    "**Date:** September 20, 2021\n",
    "\n",
    "**Description:** This script loads in MLB statcast data from every MLB registered pitch since 2015. The data is scraped from *baseballsavant.com* for each year and combined together in one large data file.\n",
    "\n",
    "**Notes:** The data is very large so it takes about an hour to download all data and save in repo.\n",
    "\n",
    "**Warnings:** \n",
    "\n",
    "**Outline:** \n",
    "  - Import Libraries\n",
    "  - Define Functions\n",
    "  - Set Directories\n",
    "  - Load & Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp_uJNflY6Mo"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouJL_ulmINKR"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import warnings\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9oM6gR-Y5fN"
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fUz-XfFIWfr"
   },
   "outputs": [],
   "source": [
    "# From pybaseball Github repo\n",
    "def validate_datestring(date_text):\n",
    "    try:\n",
    "        datetime.datetime.strptime(date_text, '%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Incorrect data format, should be YYYY-MM-DD\")\n",
    "\n",
    "def sanitize_input(start_dt, end_dt):\n",
    "    # if no dates are supplied, assume they want yesterday's data\n",
    "    # send a warning in case they wanted to specify\n",
    "    if start_dt is None and end_dt is None:\n",
    "        today = datetime.datetime.today()\n",
    "        start_dt = (today - datetime.timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "        end_dt = today.strftime(\"%Y-%m-%d\")\n",
    "        print(\"Warning: no date range supplied. Returning yesterday's Statcast data. For a different date range, try get_statcast(start_dt, end_dt).\")\n",
    "    #if only one date is supplied, assume they only want that day's stats\n",
    "    #query in this case is from date 1 to date 1\n",
    "    if start_dt is None:\n",
    "        start_dt = end_dt\n",
    "    if end_dt is None:\n",
    "        end_dt = start_dt\n",
    "    # now that both dates are not None, make sure they are valid date strings\n",
    "    validate_datestring(start_dt)\n",
    "    validate_datestring(end_dt)\n",
    "    return start_dt, end_dt\n",
    "\n",
    "def single_game_request(game_pk):\n",
    "    url = \"https://baseballsavant.mlb.com/statcast_search/csv?all=true&type=details&game_pk={game_pk}\".format(game_pk=game_pk)\n",
    "    s=requests.get(url, timeout=None).content\n",
    "    data = pd.read_csv(io.StringIO(s.decode('utf-8')))#, error_bad_lines=False) # skips 'bad lines' breaking scrapes. still testing this.\n",
    "    return data\n",
    "\n",
    "def small_request(start_dt,end_dt):\n",
    "    url = \"https://baseballsavant.mlb.com/statcast_search/csv?all=true&hfPT=&hfAB=&hfBBT=&hfPR=&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7CPO%7CS%7C=&hfSea=&hfSit=&player_type=pitcher&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=&game_date_gt={}&game_date_lt={}&team=&position=&hfRO=&home_road=&hfFlag=&metric_1=&hfInn=&min_pitches=0&min_results=0&group_by=name&sort_col=pitches&player_event_sort=h_launch_speed&sort_order=desc&min_abs=0&type=details&\".format(start_dt, end_dt)\n",
    "    s=requests.get(url, timeout=None).content\n",
    "    data = pd.read_csv(io.StringIO(s.decode('utf-8')))#, error_bad_lines=False) # skips 'bad lines' breaking scrapes. still testing this.\n",
    "    return data\n",
    "\n",
    "def large_request(start_dt,end_dt,d1,d2,step,verbose):\n",
    "    \"\"\"\n",
    "    break start and end date into smaller increments, collecting all data in small chunks and appending all results to a common dataframe\n",
    "    end_dt is the date strings for the final day of the query\n",
    "    d1 and d2 are datetime objects for first and last day of query, for doing date math\n",
    "    a third datetime object (d) will be used to increment over time for the several intermediate queries\n",
    "    \"\"\"\n",
    "    error_counter = 0 # count failed requests. If > X, break\n",
    "    no_success_msg_flag = False # a flag for passing over the success message of requests are failing\n",
    "    print(\"This is a large query, it may take a moment to complete\")\n",
    "    dataframe_list = []\n",
    "    #step = 3 # number of days per mini-query (test this later to see how large I can make this without losing data)\n",
    "    d = d1 + datetime.timedelta(days=step)\n",
    "    while d <= d2: #while intermediate query end_dt <= global query end_dt, keep looping\n",
    "        # dates before 3/15 and after 11/15 will always be offseason\n",
    "        # if these dates are detected, check if the next season is within the user's query\n",
    "        # if yes, fast-forward to the next season to avoid empty requests\n",
    "        # if no, break the loop. all useful data has been pulled.\n",
    "        if ((d.month < 4 and d.day < 15) or (d1.month > 10 and d1.day > 14)):\n",
    "            if d2.year > d.year:\n",
    "                print('Skipping offseason dates')\n",
    "                d1 = d1.replace(month=3,day=15,year=d1.year+1)\n",
    "                d = d1 + datetime.timedelta(days=step+1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        start_dt = d1.strftime('%Y-%m-%d')\n",
    "        intermediate_end_dt = d.strftime('%Y-%m-%d')\n",
    "        data = small_request(start_dt,intermediate_end_dt)\n",
    "        # append to list of dataframes if not empty or failed (failed requests have one row saying \"Error: Query Timeout\")\n",
    "        if data.shape[0] > 1:\n",
    "            dataframe_list.append(data)\n",
    "        # if it failed, retry up to three times\n",
    "        else:\n",
    "            success = 0\n",
    "            while success == 0:\n",
    "                data = small_request(start_dt,intermediate_end_dt)\n",
    "                if data.shape[0] > 1:\n",
    "                    dataframe_list.append(data)\n",
    "                    success = 1\n",
    "                else:\n",
    "                    error_counter += 1\n",
    "                if error_counter > 2:\n",
    "                    # this request is probably too large. Cut a day off of this request and make that its own separate request.\n",
    "                    # For each, append to dataframe list if successful, skip and print error message if failed\n",
    "                    tmp_end = d - datetime.timedelta(days=1)\n",
    "                    tmp_end = tmp_end.strftime('%Y-%m-%d')\n",
    "                    smaller_data_1 = small_request(start_dt, tmp_end)\n",
    "                    smaller_data_2 = small_request(intermediate_end_dt,intermediate_end_dt)\n",
    "                    if smaller_data_1.shape[0] > 1:\n",
    "                        dataframe_list.append(smaller_data_1)\n",
    "                        print(\"Completed sub-query from {} to {}\".format(start_dt,tmp_end))\n",
    "                    else:\n",
    "                        print(\"Query unsuccessful for data from {} to {}. Skipping these dates.\".format(start_dt,tmp_end))\n",
    "                    if smaller_data_2.shape[0] > 1:\n",
    "                        dataframe_list.append(smaller_data_2)\n",
    "                        print(\"Completed sub-query from {} to {}\".format(intermediate_end_dt,intermediate_end_dt))\n",
    "                    else:\n",
    "                        print(\"Query unsuccessful for data from {} to {}. Skipping these dates.\".format(intermediate_end_dt,intermediate_end_dt))\n",
    "\n",
    "                    no_success_msg_flag = True # flag for passing over the success message since this request failed\n",
    "                    error_counter = 0 # reset counter\n",
    "                    break\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            if no_success_msg_flag is False:\n",
    "                print(\"Completed sub-query from {} to {}\".format(start_dt,intermediate_end_dt))\n",
    "            else:\n",
    "                no_success_msg_flag = False # if failed, reset this flag so message will send again next iteration\n",
    "        # increment dates\n",
    "        d1 = d + datetime.timedelta(days=1)\n",
    "        d = d + datetime.timedelta(days=step+1)\n",
    "\n",
    "    # if start date > end date after being incremented, the loop captured each date's data\n",
    "    if d1 > d2:\n",
    "        pass\n",
    "    # if start date <= end date, then there are a few leftover dates to grab data for.\n",
    "    else:\n",
    "        # start_dt from the earlier loop will work, but instead of d we now want the original end_dt\n",
    "        start_dt = d1.strftime('%Y-%m-%d')\n",
    "        data = small_request(start_dt,end_dt)\n",
    "        dataframe_list.append(data)\n",
    "        if verbose:\n",
    "            print(\"Completed sub-query from {} to {}\".format(start_dt,end_dt))\n",
    "\n",
    "    # concatenate all dataframes into final result set\n",
    "    final_data = pd.concat(dataframe_list, axis=0)\n",
    "    return final_data\n",
    "\n",
    "def postprocessing(data, team):\n",
    "    #replace empty entries and 'null' strings with np.NaN\n",
    "    data.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "    data.replace(r'^null$', np.nan, regex=True, inplace = True)\n",
    "\n",
    "    # convert columns to numeric\n",
    "    not_numeric = ['sv_id', 'umpire', 'type', 'inning_topbot', 'bb_type', 'away_team', 'home_team', 'p_throws', \n",
    "                   'stand', 'game_type', 'des', 'description', 'events', 'player_name', 'game_date', 'pitch_type', 'pitch_name']\n",
    "\n",
    "    numeric_cols = ['release_speed','release_pos_x','release_pos_z','batter','pitcher','zone','hit_location','balls',\n",
    "                    'strikes','game_year','pfx_x','pfx_z','plate_x','plate_z','on_3b','on_2b','on_1b','outs_when_up','inning',\n",
    "                    'hc_x','hc_y','fielder_2','vx0','vy0','vz0','ax','ay','az','sz_top','sz_bot',\n",
    "                    'hit_distance_sc','launch_speed','launch_angle','effective_speed','release_spin_rate','release_extension',\n",
    "                    'game_pk','pitcher.1','fielder_2.1','fielder_3','fielder_4','fielder_5',\n",
    "                    'fielder_6','fielder_7','fielder_8','fielder_9','release_pos_y',\n",
    "                    'estimated_ba_using_speedangle','estimated_woba_using_speedangle','woba_value','woba_denom','babip_value',\n",
    "                    'iso_value','launch_speed_angle','at_bat_number','pitch_number','home_score','away_score','bat_score',\n",
    "                    'fld_score','post_away_score','post_home_score','post_bat_score','post_fld_score']\n",
    "\n",
    "    data[numeric_cols] = data[numeric_cols].astype(float)\n",
    "\n",
    "    # convert date col to datetime data type and sort so that this returns in an order that makes sense (by date and game)\n",
    "    data['game_date'] = pd.to_datetime(data['game_date'], format='%Y-%m-%d')\n",
    "    data = data.sort_values(['game_date', 'game_pk', 'at_bat_number', 'pitch_number'], ascending=False)\n",
    "\n",
    "    #select only pitches from a particular team\n",
    "    valid_teams = ['MIN', 'PHI', 'BAL', 'NYY', 'LAD', 'OAK', 'SEA', 'TB', 'MIL', 'MIA',\n",
    "       'KC', 'TEX', 'CHC', 'ATL', 'COL', 'HOU', 'CIN', 'LAA', 'DET', 'TOR',\n",
    "       'PIT', 'NYM', 'CLE', 'CWS', 'STL', 'WSH', 'SF', 'SD', 'BOS','ARI','ANA','WAS']\n",
    "\n",
    "    if(team in valid_teams):\n",
    "        data = data.loc[(data['home_team']==team)|(data['away_team']==team)]\n",
    "    elif(team != None):\n",
    "        raise ValueError('Error: invalid team abbreviation. Valid team names are: {}'.format(valid_teams))\n",
    "    data = data.reset_index()\n",
    "    return data\n",
    "\n",
    "def statcast(start_dt=None, end_dt=None, team=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Pulls statcast play-level data from Baseball Savant for a given date range.\n",
    "    INPUTS:\n",
    "    start_dt: YYYY-MM-DD : the first date for which you want statcast data\n",
    "    end_dt: YYYY-MM-DD : the last date for which you want statcast data\n",
    "    team: optional (defaults to None) : city abbreviation of the team you want data for (e.g. SEA or BOS)\n",
    "    If no arguments are provided, this will return yesterday's statcast data. If one date is provided, it will return that date's statcast data.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    start_dt, end_dt = sanitize_input(start_dt, end_dt)\n",
    "    # 3 days or less -> a quick one-shot request. Greater than 3 days -> break it into multiple smaller queries\n",
    "    small_query_threshold = 5\n",
    "    # inputs are valid if either both or zero dates are supplied. Not valid of only one given.\n",
    "\n",
    "\n",
    "    if start_dt and end_dt:\n",
    "        # how many days worth of data are needed?\n",
    "        date_format = \"%Y-%m-%d\"\n",
    "        d1 = datetime.datetime.strptime(start_dt, date_format)\n",
    "        d2 = datetime.datetime.strptime(end_dt, date_format)\n",
    "        days_in_query = (d2 - d1).days\n",
    "        if days_in_query <= small_query_threshold:\n",
    "            data = small_request(start_dt,end_dt)\n",
    "        else:\n",
    "            data = large_request(start_dt,end_dt,d1,d2,step=small_query_threshold,verbose=verbose)\n",
    "\n",
    "        data = postprocessing(data, team)\n",
    "        return data\n",
    "\n",
    "def statcast_single_game(game_pk, team=None):\n",
    "    \"\"\"\n",
    "    Pulls statcast play-level data from Baseball Savant for a single game,\n",
    "    identified by its MLB game ID (game_pk in statcast data)\n",
    "    INPUTS:\n",
    "    game_pk : 6-digit integer MLB game ID to retrieve\n",
    "    \"\"\"\n",
    "    data = single_game_request(game_pk)\n",
    "    data = postprocessing(data, team)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuSlU1yAcEn3"
   },
   "source": [
    "## Set Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1632179558446,
     "user": {
      "displayName": "Joel Klein",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12422283647264597477"
     },
     "user_tz": 300
    },
    "id": "a2w472eKcCRA",
    "outputId": "7268e9b6-f91a-42e6-e528-1b7e5eb50a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Remove comment from below lines, if running on Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATA_DIR = \"/content/drive/MyDrive/final-project-dl/data/statcast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOY_IZ0eZIky"
   },
   "source": [
    "## Load & Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg1_ykrYIaLi"
   },
   "outputs": [],
   "source": [
    "# Scrape all Statcast data from 2015 and export to CSVs\n",
    "\n",
    "all_15 = statcast(\"2015-04-01\",\"2015-10-04\")\n",
    "export_csv = all_15.to_csv(os.path.join(DATA_DIR, f'all_15_dataframe.csv'), \n",
    "\t\t\tindex = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2-oUe4WkyAW"
   },
   "outputs": [],
   "source": [
    "all_16 = statcast(\"2016-04-03\",\"2016-10-02\")\n",
    "export_csv = all_16.to_csv(os.path.join(DATA_DIR, f'all_16_dataframe.csv'),\n",
    "\t\t\tindex = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvZ234V9kzUA"
   },
   "outputs": [],
   "source": [
    "all_17 = statcast(\"2017-04-02\",\"2017-10-01\")\n",
    "export_csv = all_17.to_csv(os.path.join(DATA_DIR, f'all_17_dataframe.csv'), \n",
    "\t\t\tindex = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhX0S675k6eX"
   },
   "outputs": [],
   "source": [
    "all_18 = statcast(\"2018-03-29\",\"2018-10-01\")\n",
    "export_csv = all_18.to_csv(os.path.join(DATA_DIR, f'all_18_dataframe.csv'), \n",
    "\t\t\tindex = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYkuhTp_k92p"
   },
   "outputs": [],
   "source": [
    "all_19 = statcast(\"2019-03-20\",\"2019-09-29\")\n",
    "export_csv = all_19.to_csv(os.path.join(DATA_DIR, f'all_19_dataframe.csv'), \n",
    "\t\t\tindex = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxprN1lhk_mw"
   },
   "outputs": [],
   "source": [
    "all_20 = statcast(\"2020-07-23\",\"2020-09-27\")\n",
    "export_csv = all_20.to_csv(os.path.join(DATA_DIR, f'all_20_dataframe.csv'), \n",
    "\t\t\tindex = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiHGsquOlBR-"
   },
   "outputs": [],
   "source": [
    "all_21 = statcast(\"2021-04-01\",\"2021-10-03\")\n",
    "export_csv = all_21.to_csv(os.path.join(DATA_DIR, f'all_21_dataframe.csv'), \n",
    "\t\t\tindex = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Jp_uJNflY6Mo",
    "C9oM6gR-Y5fN",
    "XuSlU1yAcEn3",
    "MOY_IZ0eZIky"
   ],
   "name": "statcast_data_download.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
